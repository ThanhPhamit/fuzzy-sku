"""
OpenSearch Indexer for Japanese SKU Master Data
Optimized for fuzzy matching with Japanese text variations
"""

from opensearchpy import OpenSearch, RequestsHttpConnection
from requests_aws4auth import AWS4Auth
import boto3
import csv
import os
from datetime import datetime


class JapaneseSKUIndexer:
    def __init__(self):
        self.aws_profile = "welfan-lg-mfa"
        self.aws_region = "ap-northeast-3"
        self.endpoint = (
            "search-fuzzy-sku-ppba34qtds6ocweyl62wmgv5we.aos.ap-northeast-3.on.aws"
        )
        self.index_name = "tm-juchum"  # Changed to TM_JUCHUM index
        self.client = None

    def connect(self):
        """Connect to OpenSearch with AWS authentication"""
        try:
            print(f"ğŸ”§ Connecting with profile: {self.aws_profile}")

            session = boto3.Session(profile_name=self.aws_profile)
            credentials = session.get_credentials()

            if not credentials:
                raise Exception(
                    f"AWS credentials not found for profile: {self.aws_profile}"
                )

            awsauth = AWS4Auth(
                credentials.access_key,
                credentials.secret_key,
                self.aws_region,
                "es",
                session_token=credentials.token,
            )

            self.client = OpenSearch(
                hosts=[{"host": self.endpoint, "port": 443}],
                http_auth=awsauth,
                use_ssl=True,
                verify_certs=True,
                connection_class=RequestsHttpConnection,
                timeout=30,
            )

            # Test connection
            health = self.client.cluster.health()
            print(f"âœ… Connected! Cluster: {health['status']}")
            return True

        except Exception as e:
            print(f"âŒ Connection failed: {e}")
            return False

    def create_optimized_index(self):
        """Create index optimized for Japanese SKU fuzzy matching"""

        # Advanced mapping for Japanese text with multiple analyzers
        mapping = {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 1,
                "index.max_ngram_diff": 10,  # Increased from 7 to support longer brand names
                "refresh_interval": "1s",  # Real-time search
                "index.translog.durability": "async",
                "index.translog.sync_interval": "5s",
                "analysis": {
                    "char_filter": {
                        "normalize_chars": {
                            "type": "mapping",
                            "mappings": [
                                # Zenkaku to Hankaku numbers
                                "ï¼ => 0",
                                "ï¼‘ => 1",
                                "ï¼’ => 2",
                                "ï¼“ => 3",
                                "ï¼” => 4",
                                "ï¼• => 5",
                                "ï¼– => 6",
                                "ï¼— => 7",
                                "ï¼˜ => 8",
                                "ï¼™ => 9",
                                # Zenkaku to Hankaku alphabets
                                "ï¼¡ => A",
                                "ï¼¢ => B",
                                "ï¼£ => C",
                                "ï¼¤ => D",
                                "ï¼¥ => E",
                                "ï¼¦ => F",
                                "ï¼§ => G",
                                "ï¼¨ => H",
                                "ï¼© => I",
                                "ï¼ª => J",
                                "ï¼« => K",
                                "ï¼¬ => L",
                                "ï¼­ => M",
                                "ï¼® => N",
                                "ï¼¯ => O",
                                "ï¼° => P",
                                "ï¼± => Q",
                                "ï¼² => R",
                                "ï¼³ => S",
                                "ï¼´ => T",
                                "ï¼µ => U",
                                "ï¼¶ => V",
                                "ï¼· => W",
                                "ï¼¸ => X",
                                "ï¼¹ => Y",
                                "ï¼º => Z",
                                # Lowercase versions
                                "ï½ => a",
                                "ï½‚ => b",
                                "ï½ƒ => c",
                                "ï½„ => d",
                                "ï½… => e",
                                "ï½† => f",
                                "ï½‡ => g",
                                "ï½ˆ => h",
                                "ï½‰ => i",
                                "ï½Š => j",
                                "ï½‹ => k",
                                "ï½Œ => l",
                                "ï½ => m",
                                "ï½ => n",
                                "ï½ => o",
                                "ï½ => p",
                                "ï½‘ => q",
                                "ï½’ => r",
                                "ï½“ => s",
                                "ï½” => t",
                                "ï½• => u",
                                "ï½– => v",
                                "ï½— => w",
                                "ï½˜ => x",
                                "ï½™ => y",
                                "ï½š => z",
                                # Special characters
                                "ï¼ˆ => (",
                                "ï¼‰ => )",
                                "ï¼ => -",
                                "ã€€ => ",
                                "ï½ => ~",
                                "ãƒ» => ãƒ»",
                                "ï¼ => /",
                                "ï¼‹ => +",
                                "ï¼ => =",
                            ],
                        },
                        "katakana_hiragana": {
                            "type": "mapping",
                            "mappings": [
                                # ----- GojÅ«on -----
                                "ã‚¢ => ã‚",
                                "ã‚¤ => ã„",
                                "ã‚¦ => ã†",
                                "ã‚¨ => ãˆ",
                                "ã‚ª => ãŠ",
                                "ã‚« => ã‹",
                                "ã‚­ => ã",
                                "ã‚¯ => ã",
                                "ã‚± => ã‘",
                                "ã‚³ => ã“",
                                "ã‚µ => ã•",
                                "ã‚· => ã—",
                                "ã‚¹ => ã™",
                                "ã‚» => ã›",
                                "ã‚½ => ã",
                                "ã‚¿ => ãŸ",
                                "ãƒ => ã¡",
                                "ãƒ„ => ã¤",
                                "ãƒ† => ã¦",
                                "ãƒˆ => ã¨",
                                "ãƒŠ => ãª",
                                "ãƒ‹ => ã«",
                                "ãƒŒ => ã¬",
                                "ãƒ => ã­",
                                "ãƒ => ã®",
                                "ãƒ => ã¯",
                                "ãƒ’ => ã²",
                                "ãƒ• => ãµ",
                                "ãƒ˜ => ã¸",
                                "ãƒ› => ã»",
                                "ãƒ => ã¾",
                                "ãƒŸ => ã¿",
                                "ãƒ  => ã‚€",
                                "ãƒ¡ => ã‚",
                                "ãƒ¢ => ã‚‚",
                                "ãƒ¤ => ã‚„",
                                "ãƒ¦ => ã‚†",
                                "ãƒ¨ => ã‚ˆ",
                                "ãƒ© => ã‚‰",
                                "ãƒª => ã‚Š",
                                "ãƒ« => ã‚‹",
                                "ãƒ¬ => ã‚Œ",
                                "ãƒ­ => ã‚",
                                "ãƒ¯ => ã‚",
                                "ãƒ² => ã‚’",
                                "ãƒ³ => ã‚“",
                                # ----- Dakuten / Handakuten -----
                                "ã‚¬ => ãŒ",
                                "ã‚® => ã",
                                "ã‚° => ã",
                                "ã‚² => ã’",
                                "ã‚´ => ã”",
                                "ã‚¶ => ã–",
                                "ã‚¸ => ã˜",
                                "ã‚º => ãš",
                                "ã‚¼ => ãœ",
                                "ã‚¾ => ã",
                                "ãƒ€ => ã ",
                                "ãƒ‚ => ã¢",
                                "ãƒ… => ã¥",
                                "ãƒ‡ => ã§",
                                "ãƒ‰ => ã©",
                                "ãƒ => ã°",
                                "ãƒ“ => ã³",
                                "ãƒ– => ã¶",
                                "ãƒ™ => ã¹",
                                "ãƒœ => ã¼",
                                "ãƒ‘ => ã±",
                                "ãƒ” => ã´",
                                "ãƒ— => ã·",
                                "ãƒš => ãº",
                                "ãƒ => ã½",
                                # ----- Small kana -----
                                "ã‚¡ => ã",
                                "ã‚£ => ãƒ",
                                "ã‚¥ => ã…",
                                "ã‚§ => ã‡",
                                "ã‚© => ã‰",
                                "ãƒƒ => ã£",
                                "ãƒ£ => ã‚ƒ",
                                "ãƒ¥ => ã‚…",
                                "ãƒ§ => ã‚‡",
                                "ãƒ® => ã‚",
                                # ----- Historical kana -----
                                "ãƒ° => ã‚",
                                "ãƒ± => ã‚‘",
                                # ----- V-sounds (modern; put longer first) -----
                                "ãƒ´ã‚¡ => ã‚”ã",
                                "ãƒ´ã‚£ => ã‚”ãƒ",
                                "ãƒ´ã‚¥ => ã‚”ã…",
                                "ãƒ´ã‚§ => ã‚”ã‡",
                                "ãƒ´ã‚© => ã‚”ã‰",
                                "ãƒ´ => ã‚”",
                                # Single-codepoint VA/VI/VE/VO â†’ modern
                                "ãƒ· => ã‚”ã",
                                "ãƒ¸ => ã‚”ãƒ",
                                "ãƒ¹ => ã‚”ã‡",
                                "ãƒº => ã‚”ã‰",
                                # ----- Small KA/KE (counters; no auto-voicing) -----
                                "ãƒµ => ã‹",
                                "ãƒ¶ => ã‘",
                                # ----- Iteration marks (use kuromoji_iteration_mark if possible) -----
                                "ãƒ½ => ã‚",
                                "ãƒ¾ => ã‚",
                                # ----- Keep-as-is for product formatting -----
                                "ãƒ¼ => ãƒ¼",
                                "ãƒ» => ãƒ»",
                                # ----- Ainu small kana (Katakana Phonetic Extensions) -----
                                "ã‡° => ã",
                                "ã‡± => ã—",
                                "ã‡² => ã™",
                                "ã‡³ => ã¨",
                                "ã‡´ => ã¬",
                                "ã‡µ => ã¯",
                                "ã‡¶ => ã²",
                                "ã‡· => ãµ",
                                "ã‡·ã‚š => ã·",
                                "ã‡¸ => ã¸",
                                "ã‡¹ => ã»",
                                "ã‡º => ã‚€",
                                "ã‡» => ã‚‰",
                                "ã‡¼ => ã‚Š",
                                "ã‡½ => ã‚‹",
                                "ã‡¾ => ã‚Œ",
                                "ã‡¿ => ã‚",
                            ],
                        },
                    },
                    "tokenizer": {
                        "japanese_char_ngram": {
                            "type": "ngram",
                            "min_gram": 2,
                            "max_gram": 4,
                            "token_chars": ["letter", "digit"],
                        }
                    },
                    "analyzer": {
                        # Standard Japanese analyzer - for basic word-level matching
                        # Normalizes text and uses Kuromoji for proper Japanese tokenization
                        "japanese_standard": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_baseform",  # Convert to dictionary form
                                "kuromoji_part_of_speech",  # Filter by POS tags
                                "cjk_width",  # Normalize character width
                                "lowercase",
                            ],
                        },
                        # N-gram analyzer - for prefix/autocomplete matching
                        # Good for "as-you-type" search functionality
                        "japanese_ngram": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_baseform",
                                "cjk_width",
                                "lowercase",
                                "edge_ngram_filter",  # Creates prefix tokens (2-8 chars)
                            ],
                        },
                        # Character-level fuzzy analyzer - for typo tolerance
                        # Handles character-level variations and misspellings
                        "japanese_fuzzy": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "japanese_char_ngram",  # Character n-grams
                            "filter": [
                                "cjk_width",
                                "lowercase",
                            ],
                        },
                        # Partial word matching - for incomplete queries
                        # Allows matching parts of words within compound terms
                        "japanese_partial": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_baseform",
                                "cjk_width",
                                "lowercase",
                                "char_ngram_filter",  # Creates 2-4 char n-grams
                            ],
                        },
                        # Exact match analyzer - for precise queries
                        # Treats entire input as single token for exact matching
                        "exact_match": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "keyword",  # No tokenization, exact match
                            "filter": ["cjk_width", "lowercase"],
                        },
                        # Reading analyzer - for phonetic matching
                        # Useful for matching different writings of same pronunciation
                        "reading_analyzer": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_baseform",
                                "cjk_width",
                                "lowercase",
                            ],
                        },
                        # Synonym analyzer - for domain-specific term matching
                        # Expands queries with related medical/care product terms
                        "synonym_analyzer": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_baseform",
                                "cjk_width",
                                "lowercase",
                                "product_synonyms",  # Applies synonym mappings
                            ],
                        },
                        # Romaji analyzer - for English/ASCII input matching
                        # Converts Japanese (Hiragana/Katakana/Kanji) to Latin alphabet (Romaji)
                        "romaji_analyzer": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_baseform",
                                "kuromoji_readingform",  # Converts to Katakana reading
                                "romaji_readingform",  # Converts Katakana â†’ Romaji
                                "cjk_width",
                                "lowercase",
                            ],
                        },
                        # Pure Romaji converter - converts everything to Latin alphabet
                        # Best for cross-language matching (Japanese input â†’ English output)
                        "to_romaji_analyzer": {
                            "type": "custom",
                            "char_filter": ["normalize_chars"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_readingform",  # Kanji â†’ Katakana (ã‚·ãƒ£ãƒ¯ãƒ¼)
                                "romaji_readingform",  # Katakana â†’ Romaji (shawaa)
                                "cjk_width",
                                "lowercase",
                            ],
                        },
                        # Latin N-gram analyzer - for substring matching in Latin text
                        # KEY SOLUTION: Index "MOGU" â†’ creates n-grams: "mo", "og", "gu", "mog", "ogu", "mogu"
                        # Then search "ã‚‚ãã£ã¡" â†’ converts to "mogucchi" â†’ matches "mogu" n-gram
                        "latin_ngram_analyzer": {
                            "type": "custom",
                            "char_filter": ["normalize_chars"],
                            "tokenizer": "standard",  # Standard tokenizer for Latin text
                            "filter": [
                                "lowercase",
                                "latin_ngram_filter",  # Creates n-grams for Latin text
                            ],
                        },
                        # Romaji Edge N-gram analyzer - for prefix matching on Romaji
                        # Solves: "ã‚‚ãã£ã¡" (mogucchi) should match "MOGU" (mogu) as prefix
                        # Index: "mogucchi" â†’ edge n-grams ["m", "mo", "mog", "mogu", "moguc", "mogucc", "mogucch", "mogucchi"]
                        # Search: "mogu" â†’ matches edge n-gram "mogu"
                        "romaji_edge_ngram_analyzer": {
                            "type": "custom",
                            "char_filter": ["normalize_chars"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_readingform",  # Convert to Katakana reading
                                "romaji_readingform",  # Convert to Romaji
                                "lowercase",
                                "romaji_edge_ngram_filter",  # Create edge n-grams
                            ],
                        },
                        # Standard analyzer for Romaji search (no n-gram at search time)
                        "romaji_search_analyzer": {
                            "type": "custom",
                            "char_filter": ["normalize_chars"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_readingform",
                                "romaji_readingform",
                                "lowercase",
                            ],
                        },
                    },
                    "filter": {
                        # Edge n-gram filter - creates prefix tokens for autocomplete
                        # Generates tokens like: "ã‚·ãƒ£", "ã‚·ãƒ£ãƒ¯", "ã‚·ãƒ£ãƒ¯ãƒ¼" from "ã‚·ãƒ£ãƒ¯ãƒ¼"
                        "edge_ngram_filter": {
                            "type": "edge_ngram",
                            "min_gram": 2,
                            "max_gram": 8,
                        },
                        # Character n-gram filter - creates overlapping character sequences
                        # Generates tokens like: "ã‚·ãƒ£", "ãƒ£ãƒ¯", "ãƒ¯ãƒ¼" from "ã‚·ãƒ£ãƒ¯ãƒ¼"
                        "char_ngram_filter": {
                            "type": "ngram",
                            "min_gram": 2,
                            "max_gram": 4,
                        },
                        # Kuromoji reading form - converts Kanji to Katakana reading
                        # Example: è»Šæ¤…å­ â†’ ã‚¯ãƒ«ãƒã‚¤ã‚¹ (phonetic reading)
                        "kuromoji_readingform": {
                            "type": "kuromoji_readingform",
                            "use_romaji": False,  # First convert to Katakana
                        },
                        # Romaji reading form - converts Katakana to Latin alphabet
                        # Example: ã‚·ãƒ£ãƒ¯ãƒ¼ â†’ shawaa, ãƒˆã‚¤ãƒ¬ â†’ toire
                        "romaji_readingform": {
                            "type": "kuromoji_readingform",
                            "use_romaji": True,  # Convert to Romaji (Latin alphabet)
                        },
                        # Latin N-gram filter - creates n-grams for Latin/ASCII text
                        # Example: "MOGU" â†’ ["mo", "og", "gu", "mog", "ogu", "mogu"]
                        # This allows "mogu" (from ã‚‚ãã£ã¡) to match "MOGU"
                        "latin_ngram_filter": {
                            "type": "ngram",
                            "min_gram": 2,
                            "max_gram": 6,  # Support longer brand names
                        },
                        # Romaji Edge N-gram filter - creates prefix n-grams for Romaji
                        # Example: "mogucchi" â†’ ["m", "mo", "mog", "mogu", "moguc", "mogucc", "mogucch", "mogucchi"]
                        # Allows prefix search: "mogu" matches "mogucchi"
                        "romaji_edge_ngram_filter": {
                            "type": "edge_ngram",
                            "min_gram": 2,
                            "max_gram": 10,  # Support longer Japanese words in Romaji
                        },
                        # Product synonym filter - expands medical/care product terminology
                        # Maps related terms: "è»Šæ¤…å­" â†” "è»Šã„ã™" â†” "è»Šã‚¤ã‚¹" â†” "ã‚¦ã‚£ãƒ¼ãƒ«ãƒã‚§ã‚¢"
                        "product_synonyms": {
                            "type": "synonym",
                            "synonyms": [
                                "ä»‹è­·ç”¨ãŠã‚€ã¤,å¤§äººç”¨ãŠã‚€ã¤,å¤±ç¦ç”¨ãŠã‚€ã¤,ã‚¢ãƒ€ãƒ«ãƒˆãƒ€ã‚¤ãƒ‘ãƒ¼,ç´™ãŠã‚€ã¤",
                                "å°¿å–ã‚Šãƒ‘ãƒƒãƒ‰,å°¿ã¨ã‚Šãƒ‘ãƒƒãƒ‰,å¤±ç¦ãƒ‘ãƒƒãƒ‰,ä»‹è­·ãƒ‘ãƒƒãƒ‰",
                                "ãƒãƒ¼ã‚¿ãƒ–ãƒ«ãƒˆã‚¤ãƒ¬,ç°¡æ˜“ãƒˆã‚¤ãƒ¬,ä»‹è­·ãƒˆã‚¤ãƒ¬,ç§»å‹•å¼ãƒˆã‚¤ãƒ¬",
                                "æ¸©æ°´æ´—æµ„ä¾¿åº§,ã‚¦ã‚©ã‚·ãƒ¥ãƒ¬ãƒƒãƒˆ,ã‚·ãƒ£ãƒ¯ãƒ¼ãƒˆã‚¤ãƒ¬",
                                "ä¾¿å™¨,ä¾¿åº§,ä¾¿åº§å®¹å™¨,ãƒ™ãƒƒãƒ‰ãƒ‘ãƒ³",
                                "è»Šã„ã™,è»Šæ¤…å­,è»Šã‚¤ã‚¹,ã‚¦ã‚£ãƒ¼ãƒ«ãƒã‚§ã‚¢",
                                "æ­©è¡Œå™¨,ã‚·ãƒ«ãƒãƒ¼ã‚«ãƒ¼,ãƒ­ãƒ¬ãƒ¼ã‚¿,ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚¿,ãƒ­ãƒ¼ãƒ©ãƒˆãƒ¼ãƒ«",
                                "æ–,ã¤ãˆ,ã‚¹ãƒ†ãƒƒã‚­,æ­©è¡Œæ–",
                                "ç§»ä¹—ç”¨ãƒªãƒ•ãƒˆ,ä»‹è­·ãƒªãƒ•ãƒˆ,ãƒªãƒ•ã‚¿ãƒ¼,ã¤ã‚Šä¸Šã’ãƒªãƒ•ãƒˆ",
                                "ã‚·ãƒ‹ã‚¢ã‚«ãƒ¼,é›»å‹•ã‚·ãƒ‹ã‚¢ã‚«ãƒ¼,é›»å‹•ã‚«ãƒ¼ãƒˆ,ãƒ¢ãƒ“ãƒªãƒ†ã‚£ã‚¹ã‚¯ãƒ¼ã‚¿ãƒ¼",
                                "ä»‹è­·ãƒ™ãƒƒãƒ‰,ä»‹è­·ç”¨ãƒ™ãƒƒãƒ‰,é›»å‹•ãƒ™ãƒƒãƒ‰,ãƒªã‚¯ãƒ©ã‚¤ãƒ‹ãƒ³ã‚°ãƒ™ãƒƒãƒ‰",
                                "ä½“åœ§åˆ†æ•£ãƒãƒƒãƒˆãƒ¬ã‚¹,ã‚¨ã‚¢ãƒãƒƒãƒˆãƒ¬ã‚¹,è¤¥ç˜¡äºˆé˜²ãƒãƒƒãƒˆãƒ¬ã‚¹,è¤¥ç˜¡ãƒãƒƒãƒˆ",
                                "é›¢åºŠã‚»ãƒ³ã‚µãƒ¼,è¦‹å®ˆã‚Šã‚»ãƒ³ã‚µãƒ¼,å¾˜å¾Šã‚»ãƒ³ã‚µãƒ¼,èµ·ãä¸ŠãŒã‚Šã‚»ãƒ³ã‚µãƒ¼",
                                "ã‚·ãƒ£ãƒ¯ãƒ¼ãƒã‚§ã‚¢,å…¥æµ´ç”¨ã„ã™,å…¥æµ´æ¤…å­,é¢¨å‘‚ã„ã™",
                                "å£è…”ã‚±ã‚¢,å£è…”æ¸…æ‹­,å£è…”ç”¨ã‚¹ãƒãƒ³ã‚¸,ã‚ªãƒ¼ãƒ©ãƒ«ã‚±ã‚¢",
                                "ä½¿ã„æ¨ã¦æ‰‹è¢‹,ä½¿ã„åˆ‡ã‚Šæ‰‹è¢‹,ãƒ‹ãƒˆãƒªãƒ«æ‰‹è¢‹,ãƒ©ãƒ†ãƒƒã‚¯ã‚¹æ‰‹è¢‹,ãƒ“ãƒ‹ãƒ¼ãƒ«æ‰‹è¢‹",
                                "ãƒã‚¹ã‚¯,ã‚µãƒ¼ã‚¸ã‚«ãƒ«ãƒã‚¹ã‚¯,ä»‹è­·ç”¨ãƒã‚¹ã‚¯,ä¸ç¹”å¸ƒãƒã‚¹ã‚¯",
                                "æ¶ˆæ¯’æ¶²,ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«æ¶ˆæ¯’,é™¤èŒæ¶²,ã‚¨ã‚¿ãƒãƒ¼ãƒ«æ¶ˆæ¯’",
                                "ä½“æ¸©è¨ˆ,ãƒ‡ã‚¸ã‚¿ãƒ«ä½“æ¸©è¨ˆ,éæ¥è§¦ä½“æ¸©è¨ˆ,ã§ã“æ¸©åº¦è¨ˆ",
                                "è¡€åœ§è¨ˆ,ä¸Šè…•å¼è¡€åœ§è¨ˆ,æ‰‹é¦–å¼è¡€åœ§è¨ˆ",
                                "ãƒ‘ãƒ«ã‚¹ã‚ªã‚­ã‚·ãƒ¡ãƒ¼ã‚¿ãƒ¼,ãƒ‘ãƒ«ã‚¹ã‚ªã‚­ã‚·ãƒ¡ãƒ¼ã‚¿,è¡€ä¸­é…¸ç´ æ¿ƒåº¦è¨ˆ,SpO2è¨ˆ",
                                "ã¨ã‚ã¿å‰¤,å¢—ç²˜å‰¤,ãƒˆãƒ­ãƒŸå‰¤",
                                "æ „é¤Šè£œåŠ©é£Ÿå“,ä»‹è­·é£Ÿ,ã‚½ãƒ•ãƒˆé£Ÿ,ãƒŸã‚­ã‚µãƒ¼é£Ÿ",
                                "è‡ªå‹•,ã‚ªãƒ¼ãƒˆ,ã‚¸ãƒ‰ã‚¦,ã‚ªãƒ¼ãƒˆãƒãƒãƒƒã‚¯",
                                "ã‚»ãƒ³ã‚µãƒ¼,æ„ŸçŸ¥å™¨,ã‚»ãƒ³ã‚µ,èµ¤å¤–ç·šã‚»ãƒ³ã‚µãƒ¼",
                                "ãƒãƒ¼ã‚¿ãƒ–ãƒ«,æŒã¡é‹ã³,ç§»å‹•å¼,æºå¸¯",
                                "åºŠãšã‚Œé˜²æ­¢,è¤¥ç˜¡äºˆé˜²,ã˜ã‚‡ããã†äºˆé˜²",
                            ],
                        },
                    },
                },
            },
            "mappings": {
                "properties": {
                    # ğŸ¯ MAIN COMPOSITE SEARCH FIELD - combines skname1 + hinban + colornm + sizename
                    # This is the PRIMARY field for searching aitehinmei queries
                    "search_text": {
                        "type": "text",
                        "analyzer": "japanese_standard",  # Default analyzer for indexing
                        "search_analyzer": "japanese_partial",  # Different analyzer for search queries
                        "fields": {
                            # Exact match field - for precise queries (highest priority)
                            "exact": {"type": "text", "analyzer": "exact_match"},
                            # N-gram field - for prefix/autocomplete matching
                            "ngram": {"type": "text", "analyzer": "japanese_ngram"},
                            # Fuzzy field - for character-level typo tolerance
                            "fuzzy": {"type": "text", "analyzer": "japanese_fuzzy"},
                            # Partial field - for incomplete word matching
                            "partial": {"type": "text", "analyzer": "japanese_partial"},
                            # Synonym field - for domain-specific term expansion
                            "synonym": {"type": "text", "analyzer": "synonym_analyzer"},
                            # Romaji field - for English/ASCII cross-language search
                            "romaji": {"type": "text", "analyzer": "romaji_analyzer"},
                            # Pure Latin alphabet conversion - stores Japanese as Romaji
                            "latin": {"type": "text", "analyzer": "to_romaji_analyzer"},
                            # Latin N-gram field - KEY SOLUTION for Japanese â†’ Latin matching
                            "latin_ngram": {
                                "type": "text",
                                "analyzer": "latin_ngram_analyzer",
                            },
                            # Romaji Edge N-gram field - CRITICAL for prefix matching
                            "romaji_ngram": {
                                "type": "text",
                                "analyzer": "romaji_edge_ngram_analyzer",
                                "search_analyzer": "romaji_search_analyzer",
                            },
                            # Keyword field - for aggregations and exact filtering
                            "keyword": {"type": "keyword", "ignore_above": 256},
                        },
                    },
                    # ğŸ“‹ ORIGINAL FIELDS - returned in search results (NOT used for search)
                    # These fields are stored but not heavily indexed to save space
                    # hinban - Product code (exact match only, used for filtering)
                    "hinban": {
                        "type": "keyword",  # Keyword type for exact matching
                    },
                    # skname1 - Product name (stored for display)
                    "skname1": {
                        "type": "text",
                        "analyzer": "japanese_standard",
                        "index": False,  # Not indexed - only stored for display
                    },
                    # colorcd - Color code (NOT indexed, only stored for output)
                    "colorcd": {
                        "type": "keyword",
                        "index": False,  # ğŸ”¥ NOT indexed - saves space
                        "doc_values": True,  # Can still be returned in results
                    },
                    # colornm - Color name (stored for display)
                    "colornm": {
                        "type": "text",
                        "analyzer": "japanese_standard",
                        "index": False,  # Not indexed - only stored for display
                    },
                    # sizecd - Size code (NOT indexed, only stored for output)
                    "sizecd": {
                        "type": "keyword",
                        "index": False,  # ğŸ”¥ NOT indexed - saves space
                        "doc_values": True,  # Can still be returned in results
                    },
                    # sizename - Size name (stored for display)
                    "sizename": {
                        "type": "text",
                        "analyzer": "japanese_standard",
                        "index": False,  # Not indexed - only stored for display
                    },
                    # Timestamp field - when this record was indexed
                    "indexed_at": {"type": "date"},
                }
            },
        }

        try:
            if self.client.indices.exists(index=self.index_name):
                print(f"âš ï¸  Index '{self.index_name}' exists")
                choice = input("Delete and recreate? (y/N): ").lower()
                if choice in ["y", "yes"]:
                    self.client.indices.delete(index=self.index_name)
                    print(f"ğŸ—‘ï¸  Deleted: {self.index_name}")
                else:
                    return True

            self.client.indices.create(index=self.index_name, body=mapping)
            print(f"âœ… Created optimized index: {self.index_name}")
            return True

        except Exception as e:
            print(f"âŒ Index creation failed: {e}")
            return False

    def index_sku_data(self, csv_file="TM_JUCHUM.csv"):
        """Index TM_JUCHUM data from CSV with composite search field"""

        if not os.path.exists(csv_file):
            print(f"âŒ File not found: {csv_file}")
            return False

        try:
            print(f"ğŸ“„ Reading: {csv_file}")
            products = []

            with open(csv_file, "r", encoding="utf-8") as file:
                reader = csv.DictReader(
                    file
                )  # Use DictReader to access columns by name

                for row_id, row in enumerate(reader, start=1):
                    # Extract individual fields
                    hinban = row.get("hinban", "").strip()
                    skname1 = row.get("skname1", "").strip()
                    colorcd = row.get("colorcd", "").strip()
                    colornm = row.get("colornm", "").strip()
                    sizecd = row.get("sizecd", "").strip()
                    sizename = row.get("sizename", "").strip()

                    # ğŸ”¥ KEY CHANGE: Create composite search text
                    # Combine searchable fields (skname1, hinban, colornm, sizename)
                    # EXCLUDE colorcd and sizecd (codes - not searchable)
                    search_text = f"{skname1} {hinban} {colornm} {sizename}".strip()

                    product = {
                        # Main search field (composite)
                        "search_text": search_text,
                        # Original fields for response
                        "hinban": hinban,
                        "skname1": skname1,
                        "colorcd": colorcd,
                        "colornm": colornm,
                        "sizecd": sizecd,
                        "sizename": sizename,
                        "indexed_at": datetime.now().isoformat(),
                    }
                    products.append(product)

            print(f"ğŸ“Š Loaded {len(products)} TM_JUCHUM records")

            # Bulk index with progress tracking
            batch_size = 100
            total_indexed = 0

            for i in range(0, len(products), batch_size):
                batch = products[i : i + batch_size]
                bulk_body = []

                for idx, product in enumerate(batch, start=i + 1):
                    bulk_body.extend(
                        [
                            {
                                "index": {
                                    "_index": self.index_name,
                                    "_id": idx,  # Use sequential index as ID
                                }
                            },
                            product,
                        ]
                    )

                response = self.client.bulk(body=bulk_body)

                # Check for errors
                errors = 0
                if response.get("errors"):
                    for item in response["items"]:
                        if "index" in item and "error" in item["index"]:
                            errors += 1
                            print(
                                f"   Error ID {item['index']['_id']}: {item['index']['error']['reason']}"
                            )

                total_indexed += len(batch) - errors
                batch_num = (i // batch_size) + 1
                print(
                    f"ğŸ“ Batch {batch_num}: {len(batch) - errors}/{len(batch)} indexed (Total: {total_indexed})"
                )

            # Refresh index for immediate search
            self.client.indices.refresh(index=self.index_name)
            print(f"ğŸ‰ Successfully indexed {total_indexed} SKU records")

            # Show index statistics
            stats = self.client.indices.stats(index=self.index_name)
            doc_count = stats["indices"][self.index_name]["total"]["docs"]["count"]
            size = stats["indices"][self.index_name]["total"]["store"]["size_in_bytes"]
            print(f"ğŸ“ˆ Index stats: {doc_count} docs, {size:,} bytes")

            return True

        except Exception as e:
            print(f"âŒ Indexing failed: {e}")
            return False

    def validate_index(self):
        """Validate indexed data with sample aitehinmei searches"""
        print("\nğŸ” Validating index with sample aitehinmei queries...")

        # Test cases based on real aitehinmei examples
        test_cases = [
            "ã‚½ãƒ•ãƒˆã‚°ãƒªãƒƒãƒ— SOFT-GA ï¼ ãƒ¯ã‚¤ãƒ³",  # Example 1
            "503326ã€€KMD-B22-42-SH/ãƒ©ã‚¤ãƒˆãƒ–ãƒ«ãƒ¼",  # Example 2
            "821181PHè»¢ã³ã«ãã„ã‚·ãƒ¥ãƒ¼ã‚ºã¤ã¾å…ˆæœ‰ãƒ¯ã‚¤ãƒ³S",  # Example 3
            "964033ã€€ã‚µãƒ¼ãƒ†ã‚£ãƒ‘ãƒƒãƒ‰PROã€€Agã€€600",  # Example 4
            "2303è¶³å…ƒå¿œæ´GW603ä¸¡è¶³27ãèŒ¶",  # Example 5
            "310015  ã‚‚ããƒ”ãƒ¨ ã‚¤ã‚¨ãƒ­ãƒ¼",  # Example 6
            "ã‚«ãƒ«ã‚¬ãƒ¢ãƒ•ã‚¡ãƒ â…¡æŠ˜ç•³ã€€ãƒªãƒ¼ãƒ•æŸ„",  # Example 7
            "402921ãƒãƒ¼ã‚¿ãƒ–ãƒ«ãƒˆã‚¤ãƒ¬FX-30",  # Example 8
            "477004ã‚¢ã‚¤ã‚½ã‚«ãƒ«ã‚¼ãƒªãƒ¼ãƒã‚¤ã‚«ãƒ­ãƒªãƒ¼ã€€ãƒãƒ§ã‚³",  # Example 9
            "ã‚¢ã‚¤ã‚½ã‚«ãƒ« é«˜ã‚«ãƒ­ãƒªãƒ¼ã®ã‚„ã‚ã‚‰ã‹ã„ã”ã¯ã‚“ ç™½ãŒã‚†",  # Example 10
        ]

        for query in test_cases:
            try:
                # Test search on composite field
                response = self.client.search(
                    index=self.index_name,
                    body={
                        "query": {
                            "bool": {
                                "should": [
                                    {
                                        "term": {
                                            "hinban": {"value": query, "boost": 10.0}
                                        }
                                    },
                                    {
                                        "match": {
                                            "search_text": {
                                                "query": query,
                                                "boost": 5.0,
                                            }
                                        }
                                    },
                                    {
                                        "match": {
                                            "search_text.ngram": {
                                                "query": query,
                                                "boost": 3.0,
                                            }
                                        }
                                    },
                                    {
                                        "match": {
                                            "search_text.fuzzy": {
                                                "query": query,
                                                "boost": 2.5,
                                            }
                                        }
                                    },
                                    {
                                        "match": {
                                            "search_text.partial": {
                                                "query": query,
                                                "boost": 3.0,
                                            }
                                        }
                                    },
                                ]
                            }
                        },
                        "_source": ["hinban", "skname1", "colornm", "sizename"],
                        "size": 3,
                    },
                )

                hits = len(response["hits"]["hits"])
                total = response["hits"]["total"]["value"]
                print(
                    f"\n   Query: '{query[:50]}...' â†’ {hits} results (total: {total})"
                )

                # Show top results
                for i, hit in enumerate(response["hits"]["hits"], 1):
                    source = hit["_source"]
                    score = hit["_score"]
                    print(
                        f"      {i}. {source['hinban']} | {source['skname1']} | {source['colornm']} | {source['sizename']} (score: {score:.2f})"
                    )

            except Exception as e:
                print(f"   '{query}': Error - {e}")

        print("\nâœ… Index validation complete")

    def simple_search(self, query, max_results=10):
        """
        Enhanced search with optimized boost strategy and function_score
        Input: aitehinmei (mixed skname1 + hinban + colornm + sizename)
        Output: hinban, skname1, colorcd, colornm, sizecd, sizename
        """
        try:
            # ğŸ¯ Optimized boost strategy - prioritize Japanese-only queries
            should_queries = [
                # Japanese-only queries (highest priority)
                {"match": {"search_text": {"query": query, "boost": 8.0}}},
                {"match": {"search_text.exact": {"query": query, "boost": 7.0}}},
                {"match": {"search_text.ngram": {"query": query, "boost": 5.0}}},
                {"match": {"search_text.partial": {"query": query, "boost": 4.0}}},
                # Cross-language matching (lower priority)
                {"match": {"search_text.latin_ngram": {"query": query, "boost": 3.0}}},
                {"match": {"search_text.romaji_ngram": {"query": query, "boost": 2.5}}},
                {"match": {"search_text.romaji": {"query": query, "boost": 2.5}}},
                {"match": {"search_text.latin": {"query": query, "boost": 2.5}}},
                # Fallback strategies (lowest priority)
                {"match": {"search_text.fuzzy": {"query": query, "boost": 2.0}}},
                {"match": {"search_text.synonym": {"query": query, "boost": 1.5}}},
            ]

            response = self.client.search(
                index=self.index_name,
                body={
                    "query": {
                        "function_score": {
                            "query": {
                                "bool": {
                                    "should": should_queries,
                                    "minimum_should_match": "30%",
                                }
                            },
                            "functions": [
                                # Exact hinban match gets highest boost
                                {
                                    "filter": {"term": {"hinban": query}},
                                    "weight": 10.0,
                                },
                            ],
                            "score_mode": "sum",
                            "boost_mode": "multiply",
                        }
                    },
                    # ğŸ“‹ Return all fields needed for output
                    "_source": [
                        "hinban",
                        "skname1",
                        "colorcd",
                        "colornm",
                        "sizecd",
                        "sizename",
                    ],
                    "highlight": {
                        "fields": {
                            "search_text": {},
                            "search_text.exact": {},
                            "search_text.ngram": {},
                        },
                        "pre_tags": ["<mark>"],
                        "post_tags": ["</mark>"],
                    },
                    "size": max_results,
                },
            )

            hits = response["hits"]["hits"]
            total = response["hits"]["total"]["value"]

            print(f"\nğŸ” Search: '{query}'")
            print(f"ğŸ“Š Found: {len(hits)} results (total: {total})")
            print(
                f"{'#':<4} {'hinban':<12} {'skname1':<35} {'colorcd':<10} {'colornm':<15} {'sizecd':<10} {'sizename':<12} {'score':<8}"
            )
            print("-" * 130)

            for i, hit in enumerate(hits, 1):
                source = hit["_source"]
                score = hit["_score"]
                highlights = hit.get("highlight", {})
                matched_fields = ", ".join(highlights.keys()) if highlights else "N/A"

                print(
                    f"{i:<4} "
                    f"{source.get('hinban', ''):<12} "
                    f"{source.get('skname1', ''):<35} "
                    f"{source.get('colorcd', ''):<10} "
                    f"{source.get('colornm', ''):<15} "
                    f"{source.get('sizecd', ''):<10} "
                    f"{source.get('sizename', ''):<12} "
                    f"{score:<8.2f}"
                )

                # Show matched fields for debugging
                if highlights:
                    print(f"       â””â”€ Matched: {matched_fields}")

            return hits

        except Exception as e:
            print(f"âŒ Search failed: {e}")
            import traceback

            traceback.print_exc()
            return []


def main():
    """Main indexing process for TM_JUCHUM data"""
    print("ğŸš€ TM_JUCHUM Data Indexer (aitehinmei search)")
    print("=" * 60)
    print("ğŸ“‹ Optimized for:")
    print("   - Search with aitehinmei (mixed skname1+hinban+colornm+sizename)")
    print("   - Japanese text variations (å…¨è§’/åŠè§’)")
    print("   - Kanji/Hiragana/Katakana fuzzy matching")
    print("   - Return: hinban, skname1, colorcd, colornm, sizecd, sizename")
    print("   - NOT indexed: colorcd, sizecd (codes only for output)")
    print("")

    indexer = JapaneseSKUIndexer()

    # Step 1: Connect
    if not indexer.connect():
        print("ğŸ’¥ Connection failed. Check troubleshooter.")
        return

    # Step 2: Create optimized index
    if not indexer.create_optimized_index():
        print("ğŸ’¥ Index creation failed.")
        return

    # Step 3: Index TM_JUCHUM data
    if not indexer.index_sku_data("TM_JUCHUM.csv"):
        print("ğŸ’¥ Data indexing failed.")
        return

    # Step 4: Validate
    indexer.validate_index()

    print("\nğŸ‰ Indexing complete! Ready for aitehinmei search testing.")
    print("\nğŸ’¡ Example queries:")
    print("   - ã‚½ãƒ•ãƒˆã‚°ãƒªãƒƒãƒ— SOFT-GA ï¼ ãƒ¯ã‚¤ãƒ³")
    print("   - 503326ã€€KMD-B22-42-SH/ãƒ©ã‚¤ãƒˆãƒ–ãƒ«ãƒ¼")
    print("   - 821181PHè»¢ã³ã«ãã„ã‚·ãƒ¥ãƒ¼ã‚ºã¤ã¾å…ˆæœ‰ãƒ¯ã‚¤ãƒ³S")
    print("   - 310015  ã‚‚ããƒ”ãƒ¨ ã‚¤ã‚¨ãƒ­ãƒ¼")

    # Optional: Interactive search mode
    while True:
        try:
            query = input("\nğŸ” Enter aitehinmei query (or 'quit' to exit): ").strip()
            if query.lower() in ["quit", "exit", "q", ""]:
                break
            indexer.simple_search(query)
        except KeyboardInterrupt:
            break

    print("ğŸ‘‹ Session ended.")


if __name__ == "__main__":
    main()
