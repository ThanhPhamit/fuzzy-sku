"""
OpenSearch Indexer for Japanese SKU Master Data
Optimized for fuzzy matching with Japanese text variations
"""

from opensearchpy import OpenSearch, RequestsHttpConnection
from requests_aws4auth import AWS4Auth
import boto3
import csv
import os
from datetime import datetime


class JapaneseSKUIndexer:
    def __init__(self):
        self.aws_profile = "welfan-lg-mfa"
        self.aws_region = "ap-northeast-3"
        self.endpoint = (
            "search-fuzzy-sku-ppba34qtds6ocweyl62wmgv5we.aos.ap-northeast-3.on.aws"
        )
        self.index_name = "sku-master"
        self.client = None

    def connect(self):
        """Connect to OpenSearch with AWS authentication"""
        try:
            print(f"üîß Connecting with profile: {self.aws_profile}")

            session = boto3.Session(profile_name=self.aws_profile)
            credentials = session.get_credentials()

            if not credentials:
                raise Exception(
                    f"AWS credentials not found for profile: {self.aws_profile}"
                )

            awsauth = AWS4Auth(
                credentials.access_key,
                credentials.secret_key,
                self.aws_region,
                "es",
                session_token=credentials.token,
            )

            self.client = OpenSearch(
                hosts=[{"host": self.endpoint, "port": 443}],
                http_auth=awsauth,
                use_ssl=True,
                verify_certs=True,
                connection_class=RequestsHttpConnection,
                timeout=30,
            )

            # Test connection
            health = self.client.cluster.health()
            print(f"‚úÖ Connected! Cluster: {health['status']}")
            return True

        except Exception as e:
            print(f"‚ùå Connection failed: {e}")
            return False

    def create_optimized_index(self):
        """Create index optimized for Japanese SKU fuzzy matching"""

        # Advanced mapping for Japanese text with multiple analyzers
        mapping = {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 1,
                "index.max_ngram_diff": 7,
                "analysis": {
                    "char_filter": {
                        "normalize_chars": {
                            "type": "mapping",
                            "mappings": [
                                # Zenkaku to Hankaku numbers
                                "Ôºê => 0",
                                "Ôºë => 1",
                                "Ôºí => 2",
                                "Ôºì => 3",
                                "Ôºî => 4",
                                "Ôºï => 5",
                                "Ôºñ => 6",
                                "Ôºó => 7",
                                "Ôºò => 8",
                                "Ôºô => 9",
                                # Zenkaku to Hankaku alphabets
                                "Ôº° => A",
                                "Ôº¢ => B",
                                "Ôº£ => C",
                                "Ôº§ => D",
                                "Ôº• => E",
                                "Ôº¶ => F",
                                "Ôºß => G",
                                "Ôº® => H",
                                "Ôº© => I",
                                "Ôº™ => J",
                                "Ôº´ => K",
                                "Ôº¨ => L",
                                "Ôº≠ => M",
                                "ÔºÆ => N",
                                "ÔºØ => O",
                                "Ôº∞ => P",
                                "Ôº± => Q",
                                "Ôº≤ => R",
                                "Ôº≥ => S",
                                "Ôº¥ => T",
                                "Ôºµ => U",
                                "Ôº∂ => V",
                                "Ôº∑ => W",
                                "Ôº∏ => X",
                                "Ôºπ => Y",
                                "Ôº∫ => Z",
                                # Lowercase versions
                                "ÔΩÅ => a",
                                "ÔΩÇ => b",
                                "ÔΩÉ => c",
                                "ÔΩÑ => d",
                                "ÔΩÖ => e",
                                "ÔΩÜ => f",
                                "ÔΩá => g",
                                "ÔΩà => h",
                                "ÔΩâ => i",
                                "ÔΩä => j",
                                "ÔΩã => k",
                                "ÔΩå => l",
                                "ÔΩç => m",
                                "ÔΩé => n",
                                "ÔΩè => o",
                                "ÔΩê => p",
                                "ÔΩë => q",
                                "ÔΩí => r",
                                "ÔΩì => s",
                                "ÔΩî => t",
                                "ÔΩï => u",
                                "ÔΩñ => v",
                                "ÔΩó => w",
                                "ÔΩò => x",
                                "ÔΩô => y",
                                "ÔΩö => z",
                                # Special characters
                                "Ôºà => (",
                                "Ôºâ => )",
                                "Ôºç => -",
                                "„ÄÄ => ",
                                "ÔΩû => ~",
                                "„Éª => „Éª",
                                "Ôºè => /",
                                "Ôºã => +",
                                "Ôºù => =",
                            ],
                        },
                        "katakana_hiragana": {
                            "type": "mapping",
                            "mappings": [
                                # ----- Goj≈´on -----
                                "„Ç¢ => „ÅÇ",
                                "„Ç§ => „ÅÑ",
                                "„Ç¶ => „ÅÜ",
                                "„Ç® => „Åà",
                                "„Ç™ => „Åä",
                                "„Ç´ => „Åã",
                                "„Ç≠ => „Åç",
                                "„ÇØ => „Åè",
                                "„Ç± => „Åë",
                                "„Ç≥ => „Åì",
                                "„Çµ => „Åï",
                                "„Ç∑ => „Åó",
                                "„Çπ => „Åô",
                                "„Çª => „Åõ",
                                "„ÇΩ => „Åù",
                                "„Çø => „Åü",
                                "„ÉÅ => „Å°",
                                "„ÉÑ => „Å§",
                                "„ÉÜ => „Å¶",
                                "„Éà => „Å®",
                                "„Éä => „Å™",
                                "„Éã => „Å´",
                                "„Éå => „Å¨",
                                "„Éç => „Å≠",
                                "„Éé => „ÅÆ",
                                "„Éè => „ÅØ",
                                "„Éí => „Å≤",
                                "„Éï => „Åµ",
                                "„Éò => „Å∏",
                                "„Éõ => „Åª",
                                "„Éû => „Åæ",
                                "„Éü => „Åø",
                                "„É† => „ÇÄ",
                                "„É° => „ÇÅ",
                                "„É¢ => „ÇÇ",
                                "„É§ => „ÇÑ",
                                "„É¶ => „ÇÜ",
                                "„É® => „Çà",
                                "„É© => „Çâ",
                                "„É™ => „Çä",
                                "„É´ => „Çã",
                                "„É¨ => „Çå",
                                "„É≠ => „Çç",
                                "„ÉØ => „Çè",
                                "„É≤ => „Çí",
                                "„É≥ => „Çì",
                                # ----- Dakuten / Handakuten -----
                                "„Ç¨ => „Åå",
                                "„ÇÆ => „Åé",
                                "„Ç∞ => „Åê",
                                "„Ç≤ => „Åí",
                                "„Ç¥ => „Åî",
                                "„Ç∂ => „Åñ",
                                "„Ç∏ => „Åò",
                                "„Ç∫ => „Åö",
                                "„Çº => „Åú",
                                "„Çæ => „Åû",
                                "„ÉÄ => „Å†",
                                "„ÉÇ => „Å¢",
                                "„ÉÖ => „Å•",
                                "„Éá => „Åß",
                                "„Éâ => „Å©",
                                "„Éê => „Å∞",
                                "„Éì => „Å≥",
                                "„Éñ => „Å∂",
                                "„Éô => „Åπ",
                                "„Éú => „Åº",
                                "„Éë => „Å±",
                                "„Éî => „Å¥",
                                "„Éó => „Å∑",
                                "„Éö => „Å∫",
                                "„Éù => „ÅΩ",
                                # ----- Small kana -----
                                "„Ç° => „ÅÅ",
                                "„Ç£ => „ÅÉ",
                                "„Ç• => „ÅÖ",
                                "„Çß => „Åá",
                                "„Ç© => „Åâ",
                                "„ÉÉ => „Å£",
                                "„É£ => „ÇÉ",
                                "„É• => „ÇÖ",
                                "„Éß => „Çá",
                                "„ÉÆ => „Çé",
                                # ----- Historical kana -----
                                "„É∞ => „Çê",
                                "„É± => „Çë",
                                # ----- V-sounds (modern; put longer first) -----
                                "„É¥„Ç° => „Çî„ÅÅ",
                                "„É¥„Ç£ => „Çî„ÅÉ",
                                "„É¥„Ç• => „Çî„ÅÖ",
                                "„É¥„Çß => „Çî„Åá",
                                "„É¥„Ç© => „Çî„Åâ",
                                "„É¥ => „Çî",
                                # Single-codepoint VA/VI/VE/VO ‚Üí modern
                                "„É∑ => „Çî„ÅÅ",
                                "„É∏ => „Çî„ÅÉ",
                                "„Éπ => „Çî„Åá",
                                "„É∫ => „Çî„Åâ",
                                # ----- Small KA/KE (counters; no auto-voicing) -----
                                "„Éµ => „Åã",
                                "„É∂ => „Åë",
                                # ----- Iteration marks (use kuromoji_iteration_mark if possible) -----
                                "„ÉΩ => „Çù",
                                "„Éæ => „Çû",
                                # ----- Keep-as-is for product formatting -----
                                "„Éº => „Éº",
                                "„Éª => „Éª",
                                # ----- Ainu small kana (Katakana Phonetic Extensions) -----
                                "„á∞ => „Åè",
                                "„á± => „Åó",
                                "„á≤ => „Åô",
                                "„á≥ => „Å®",
                                "„á¥ => „Å¨",
                                "„áµ => „ÅØ",
                                "„á∂ => „Å≤",
                                "„á∑ => „Åµ",
                                "„á∑„Çö => „Å∑",
                                "„á∏ => „Å∏",
                                "„áπ => „Åª",
                                "„á∫ => „ÇÄ",
                                "„áª => „Çâ",
                                "„áº => „Çä",
                                "„áΩ => „Çã",
                                "„áæ => „Çå",
                                "„áø => „Çç",
                            ],
                        },
                    },
                    "tokenizer": {
                        "japanese_char_ngram": {
                            "type": "ngram",
                            "min_gram": 2,
                            "max_gram": 4,
                            "token_chars": ["letter", "digit"],
                        }
                    },
                    "analyzer": {
                        # Standard Japanese analyzer - for basic word-level matching
                        # Normalizes text and uses Kuromoji for proper Japanese tokenization
                        "japanese_standard": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_baseform",  # Convert to dictionary form
                                "kuromoji_part_of_speech",  # Filter by POS tags
                                "cjk_width",  # Normalize character width
                                "lowercase",
                            ],
                        },
                        # N-gram analyzer - for prefix/autocomplete matching
                        # Good for "as-you-type" search functionality
                        "japanese_ngram": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_baseform",
                                "cjk_width",
                                "lowercase",
                                "edge_ngram_filter",  # Creates prefix tokens (2-8 chars)
                            ],
                        },
                        # Character-level fuzzy analyzer - for typo tolerance
                        # Handles character-level variations and misspellings
                        "japanese_fuzzy": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "japanese_char_ngram",  # Character n-grams
                            "filter": [
                                "cjk_width",
                                "lowercase",
                            ],
                        },
                        # Partial word matching - for incomplete queries
                        # Allows matching parts of words within compound terms
                        "japanese_partial": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_baseform",
                                "cjk_width",
                                "lowercase",
                                "char_ngram_filter",  # Creates 2-4 char n-grams
                            ],
                        },
                        # Exact match analyzer - for precise queries
                        # Treats entire input as single token for exact matching
                        "exact_match": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "keyword",  # No tokenization, exact match
                            "filter": ["cjk_width", "lowercase"],
                        },
                        # Reading analyzer - for phonetic matching
                        # Useful for matching different writings of same pronunciation
                        "reading_analyzer": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_baseform",
                                "cjk_width",
                                "lowercase",
                            ],
                        },
                        # Synonym analyzer - for domain-specific term matching
                        # Expands queries with related medical/care product terms
                        "synonym_analyzer": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_baseform",
                                "cjk_width",
                                "lowercase",
                                "product_synonyms",  # Applies synonym mappings
                            ],
                        },
                        # Romaji analyzer - for English/ASCII input matching
                        # Converts Japanese to ASCII for cross-language search
                        "romaji_analyzer": {
                            "type": "custom",
                            "char_filter": ["normalize_chars"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_baseform",
                                "cjk_width",
                                "lowercase",
                                "asciifolding",  # Converts to ASCII equivalents
                            ],
                        },
                    },
                    "filter": {
                        # Edge n-gram filter - creates prefix tokens for autocomplete
                        # Generates tokens like: "„Ç∑„É£", "„Ç∑„É£„ÉØ", "„Ç∑„É£„ÉØ„Éº" from "„Ç∑„É£„ÉØ„Éº"
                        "edge_ngram_filter": {
                            "type": "edge_ngram",
                            "min_gram": 2,
                            "max_gram": 8,
                        },
                        # Character n-gram filter - creates overlapping character sequences
                        # Generates tokens like: "„Ç∑„É£", "„É£„ÉØ", "„ÉØ„Éº" from "„Ç∑„É£„ÉØ„Éº"
                        "char_ngram_filter": {
                            "type": "ngram",
                            "min_gram": 2,
                            "max_gram": 4,
                        },
                        # ASCII folding filter - converts accented/Japanese chars to ASCII
                        # Helps with cross-language matching („Éà„Ç§„É¨ ‚Üí toilet)
                        "asciifolding": {
                            "type": "asciifolding",
                            "preserve_original": True,  # Keep both original and ASCII versions
                        },
                        # Product synonym filter - expands medical/care product terminology
                        # Maps related terms: "ËªäÊ§ÖÂ≠ê" ‚Üî "Ëªä„ÅÑ„Åô" ‚Üî "Ëªä„Ç§„Çπ" ‚Üî "„Ç¶„Ç£„Éº„É´„ÉÅ„Çß„Ç¢"
                        "product_synonyms": {
                            "type": "synonym",
                            "synonyms": [
                                "‰ªãË≠∑Áî®„Åä„ÇÄ„Å§,Â§ß‰∫∫Áî®„Åä„ÇÄ„Å§,Â§±Á¶ÅÁî®„Åä„ÇÄ„Å§,„Ç¢„ÉÄ„É´„Éà„ÉÄ„Ç§„Éë„Éº,Á¥ô„Åä„ÇÄ„Å§",
                                "Â∞øÂèñ„Çä„Éë„ÉÉ„Éâ,Â∞ø„Å®„Çä„Éë„ÉÉ„Éâ,Â§±Á¶Å„Éë„ÉÉ„Éâ,‰ªãË≠∑„Éë„ÉÉ„Éâ",
                                "„Éù„Éº„Çø„Éñ„É´„Éà„Ç§„É¨,Á∞°Êòì„Éà„Ç§„É¨,‰ªãË≠∑„Éà„Ç§„É¨,ÁßªÂãïÂºè„Éà„Ç§„É¨",
                                "Ê∏©Ê∞¥Ê¥óÊµÑ‰æøÂ∫ß,„Ç¶„Ç©„Ç∑„É•„É¨„ÉÉ„Éà,„Ç∑„É£„ÉØ„Éº„Éà„Ç§„É¨",
                                "‰æøÂô®,‰æøÂ∫ß,‰æøÂ∫ßÂÆπÂô®,„Éô„ÉÉ„Éâ„Éë„É≥",
                                "Ëªä„ÅÑ„Åô,ËªäÊ§ÖÂ≠ê,Ëªä„Ç§„Çπ,„Ç¶„Ç£„Éº„É´„ÉÅ„Çß„Ç¢",
                                "Ê≠©Ë°åÂô®,„Ç∑„É´„Éê„Éº„Ç´„Éº,„É≠„É¨„Éº„Çø,„É≠„Éº„É©„Éº„Çø,„É≠„Éº„É©„Éà„Éº„É´",
                                "Êùñ,„Å§„Åà,„Çπ„ÉÜ„ÉÉ„Ç≠,Ê≠©Ë°åÊùñ",
                                "Áßª‰πóÁî®„É™„Éï„Éà,‰ªãË≠∑„É™„Éï„Éà,„É™„Éï„Çø„Éº,„Å§„Çä‰∏ä„Åí„É™„Éï„Éà",
                                "„Ç∑„Éã„Ç¢„Ç´„Éº,ÈõªÂãï„Ç∑„Éã„Ç¢„Ç´„Éº,ÈõªÂãï„Ç´„Éº„Éà,„É¢„Éì„É™„ÉÜ„Ç£„Çπ„ÇØ„Éº„Çø„Éº",
                                "‰ªãË≠∑„Éô„ÉÉ„Éâ,‰ªãË≠∑Áî®„Éô„ÉÉ„Éâ,ÈõªÂãï„Éô„ÉÉ„Éâ,„É™„ÇØ„É©„Ç§„Éã„É≥„Ç∞„Éô„ÉÉ„Éâ",
                                "‰ΩìÂúßÂàÜÊï£„Éû„ÉÉ„Éà„É¨„Çπ,„Ç®„Ç¢„Éû„ÉÉ„Éà„É¨„Çπ,Ë§•Áò°‰∫àÈò≤„Éû„ÉÉ„Éà„É¨„Çπ,Ë§•Áò°„Éû„ÉÉ„Éà",
                                "Èõ¢Â∫ä„Çª„É≥„Çµ„Éº,Ë¶ãÂÆà„Çä„Çª„É≥„Çµ„Éº,ÂæòÂæä„Çª„É≥„Çµ„Éº,Ëµ∑„Åç‰∏ä„Åå„Çä„Çª„É≥„Çµ„Éº",
                                "„Ç∑„É£„ÉØ„Éº„ÉÅ„Çß„Ç¢,ÂÖ•Êµ¥Áî®„ÅÑ„Åô,ÂÖ•Êµ¥Ê§ÖÂ≠ê,È¢®ÂëÇ„ÅÑ„Åô",
                                "Âè£ËÖî„Ç±„Ç¢,Âè£ËÖîÊ∏ÖÊã≠,Âè£ËÖîÁî®„Çπ„Éù„É≥„Ç∏,„Ç™„Éº„É©„É´„Ç±„Ç¢",
                                "‰Ωø„ÅÑÊç®„Å¶ÊâãË¢ã,‰Ωø„ÅÑÂàá„ÇäÊâãË¢ã,„Éã„Éà„É™„É´ÊâãË¢ã,„É©„ÉÜ„ÉÉ„ÇØ„ÇπÊâãË¢ã,„Éì„Éã„Éº„É´ÊâãË¢ã",
                                "„Éû„Çπ„ÇØ,„Çµ„Éº„Ç∏„Ç´„É´„Éû„Çπ„ÇØ,‰ªãË≠∑Áî®„Éû„Çπ„ÇØ,‰∏çÁπîÂ∏É„Éû„Çπ„ÇØ",
                                "Ê∂àÊØíÊ∂≤,„Ç¢„É´„Ç≥„Éº„É´Ê∂àÊØí,Èô§ËèåÊ∂≤,„Ç®„Çø„Éé„Éº„É´Ê∂àÊØí",
                                "‰ΩìÊ∏©Ë®à,„Éá„Ç∏„Çø„É´‰ΩìÊ∏©Ë®à,ÈùûÊé•Ëß¶‰ΩìÊ∏©Ë®à,„Åß„ÅìÊ∏©Â∫¶Ë®à",
                                "Ë°ÄÂúßË®à,‰∏äËÖïÂºèË°ÄÂúßË®à,ÊâãÈ¶ñÂºèË°ÄÂúßË®à",
                                "„Éë„É´„Çπ„Ç™„Ç≠„Ç∑„É°„Éº„Çø„Éº,„Éë„É´„Çπ„Ç™„Ç≠„Ç∑„É°„Éº„Çø,Ë°Ä‰∏≠ÈÖ∏Á¥†ÊøÉÂ∫¶Ë®à,SpO2Ë®à",
                                "„Å®„Çç„ÅøÂâ§,Â¢óÁ≤òÂâ§,„Éà„É≠„ÉüÂâ§",
                                "Ê†ÑÈ§äË£úÂä©È£üÂìÅ,‰ªãË≠∑È£ü,„ÇΩ„Éï„ÉàÈ£ü,„Éü„Ç≠„Çµ„ÉºÈ£ü",
                                "Ëá™Âãï,„Ç™„Éº„Éà,„Ç∏„Éâ„Ç¶,„Ç™„Éº„Éà„Éû„ÉÅ„ÉÉ„ÇØ",
                                "„Çª„É≥„Çµ„Éº,ÊÑüÁü•Âô®,„Çª„É≥„Çµ,Ëµ§Â§ñÁ∑ö„Çª„É≥„Çµ„Éº",
                                "„Éù„Éº„Çø„Éñ„É´,ÊåÅ„Å°ÈÅã„Å≥,ÁßªÂãïÂºè,Êê∫Â∏Ø",
                                "Â∫ä„Åö„ÇåÈò≤Ê≠¢,Ë§•Áò°‰∫àÈò≤,„Åò„Çá„Åè„Åù„ÅÜ‰∫àÈò≤",
                            ],
                        },
                    },
                },
            },
            "mappings": {
                "properties": {
                    # Primary ID field - integer identifier for each SKU
                    "id": {"type": "integer"},
                    # Main SKU name field with multiple sub-fields for different search strategies
                    "sku_name": {
                        "type": "text",
                        "analyzer": "japanese_standard",  # Default analyzer for indexing
                        "search_analyzer": "japanese_partial",  # Different analyzer for search queries
                        "fields": {
                            # Exact match field - for precise queries (highest priority)
                            "exact": {"type": "text", "analyzer": "exact_match"},
                            # N-gram field - for prefix/autocomplete matching
                            "ngram": {"type": "text", "analyzer": "japanese_ngram"},
                            # Fuzzy field - for character-level typo tolerance
                            "fuzzy": {"type": "text", "analyzer": "japanese_fuzzy"},
                            # Partial field - for incomplete word matching
                            "partial": {"type": "text", "analyzer": "japanese_partial"},
                            # Synonym field - for domain-specific term expansion
                            "synonym": {"type": "text", "analyzer": "synonym_analyzer"},
                            # Romaji field - for English/ASCII cross-language search
                            "romaji": {"type": "text", "analyzer": "romaji_analyzer"},
                            # Keyword field - for aggregations and exact filtering
                            "keyword": {"type": "keyword", "ignore_above": 256},
                        },
                    },
                    # Original text field - preserves raw input for reference
                    "original_text": {
                        "type": "text",
                        "analyzer": "japanese_standard",
                        "fields": {"keyword": {"type": "keyword", "ignore_above": 256}},
                    },
                    # Reading field - for phonetic variations and pronunciation matching
                    "reading": {
                        "type": "text",
                        "analyzer": "reading_analyzer",
                        "search_analyzer": "japanese_fuzzy",  # Use fuzzy for reading searches
                        "fields": {
                            "keyword": {"type": "keyword", "ignore_above": 256},
                            "romaji": {"type": "text", "analyzer": "romaji_analyzer"},
                        },
                    },
                    # Category field - for filtering and grouping products
                    "category": {"type": "keyword"},
                    # Timestamp fields - for data management and versioning
                    "created_at": {"type": "date"},
                    "updated_at": {"type": "date"},
                }
            },
        }

        try:
            if self.client.indices.exists(index=self.index_name):
                print(f"‚ö†Ô∏è  Index '{self.index_name}' exists")
                choice = input("Delete and recreate? (y/N): ").lower()
                if choice in ["y", "yes"]:
                    self.client.indices.delete(index=self.index_name)
                    print(f"üóëÔ∏è  Deleted: {self.index_name}")
                else:
                    return True

            self.client.indices.create(index=self.index_name, body=mapping)
            print(f"‚úÖ Created optimized index: {self.index_name}")
            return True

        except Exception as e:
            print(f"‚ùå Index creation failed: {e}")
            return False

    def index_sku_data(self, csv_file="TM_SYOHIN_202509302313.csv"):
        """Index SKU master data from CSV with batch processing"""

        if not os.path.exists(csv_file):
            print(f"‚ùå File not found: {csv_file}")
            return False

        try:
            print(f"üìÑ Reading: {csv_file}")
            products = []

            with open(csv_file, "r", encoding="utf-8") as file:
                reader = csv.reader(file)
                header = next(reader, None)
                print(f"Header: {header}")

                for row_id, row in enumerate(reader, start=1):
                    if row and row[0].strip():
                        sku_name = row[0].strip()

                        # Extract product info for better categorization
                        category = "unknown"
                        if any(x in sku_name for x in ["‰æøÂ∫ß", "„Éà„Ç§„É¨", "KX", "FX"]):
                            category = "toilet_products"
                        elif any(x in sku_name for x in ["ÊöñÊàø", "Ê∏©Ê∞¥"]):
                            category = "heating_products"
                        elif any(x in sku_name for x in ["„Éù„Éº„Çø„Éñ„É´", "„Éï„É¨„Éº„É†"]):
                            category = "portable_products"

                        product = {
                            "id": row_id,
                            "sku_name": sku_name,
                            "original_text": sku_name,
                            "category": category,
                            "created_at": datetime.now().isoformat(),
                            "updated_at": datetime.now().isoformat(),
                        }
                        products.append(product)

            print(f"üìä Loaded {len(products)} SKU records")

            # Bulk index with progress tracking
            batch_size = 100
            total_indexed = 0

            for i in range(0, len(products), batch_size):
                batch = products[i : i + batch_size]
                bulk_body = []

                for product in batch:
                    bulk_body.extend(
                        [
                            {
                                "index": {
                                    "_index": self.index_name,
                                    "_id": product["id"],
                                }
                            },
                            product,
                        ]
                    )

                response = self.client.bulk(body=bulk_body)

                # Check for errors
                errors = 0
                if response.get("errors"):
                    for item in response["items"]:
                        if "index" in item and "error" in item["index"]:
                            errors += 1
                            print(
                                f"   Error ID {item['index']['_id']}: {item['index']['error']['reason']}"
                            )

                total_indexed += len(batch) - errors
                batch_num = (i // batch_size) + 1
                print(
                    f"üìù Batch {batch_num}: {len(batch) - errors}/{len(batch)} indexed (Total: {total_indexed})"
                )

            # Refresh index for immediate search
            self.client.indices.refresh(index=self.index_name)
            print(f"üéâ Successfully indexed {total_indexed} SKU records")

            # Show index statistics
            stats = self.client.indices.stats(index=self.index_name)
            doc_count = stats["indices"][self.index_name]["total"]["docs"]["count"]
            size = stats["indices"][self.index_name]["total"]["store"]["size_in_bytes"]
            print(f"üìà Index stats: {doc_count} docs, {size:,} bytes")

            return True

        except Exception as e:
            print(f"‚ùå Indexing failed: {e}")
            return False

    def validate_index(self):
        """Validate indexed data with sample searches"""
        print("\nüîç Validating index with sample searches...")

        test_cases = [
            # Exact matches - should find exact products
            "FX-1",  # Exact SKU code
            "KX-SDR",  # Another exact SKU code
            "„Éù„Éº„Çø„Éñ„É´„Éà„Ç§„É¨EX-TÂûã",  # Full product name
            # Japanese variations - test normalization
            "‰æøÂ∫ß",  # Common toilet seat term
            "ÊöñÊàø",  # Heating term
            "„Ç∑„É£„ÉØ„Éº„Éô„É≥„ÉÅ",  # Shower bench (katakana)
            "„Åó„ÇÉ„Çè„Éº„Åπ„Çì„Å°",  # Same in hiragana (should match)
            "ÔΩºÔΩ¨ÔæúÔΩ∞ÔæçÔæûÔæùÔæÅ",  # Half-width katakana (should normalize)
            # Partial/typo scenarios - fuzzy matching
            "FX1",  # Missing hyphen
            "KX SDR",  # Space instead of hyphen
            "ÔæéÔæüÔΩ∞ÔæÄÔæåÔæûÔæô",  # Partial katakana
            "„Éù„Éº„Çø„Éñ„É´",  # Full katakana version
            "„Éà„Ç§„É¨",  # Generic toilet term
            # Complex product names from CSV
            "ÈõªÂãï‰æøÂ∫ßÊòáÈôçÊ©ü",  # Electric toilet seat lift
            "Âê∏ÁùÄ„Åô„Åπ„ÇäÊ≠¢„ÇÅ„Éû„ÉÉ„Éà",  # Anti-slip mat
            "Êú®Ë£ΩÁéÑÈñ¢Âè∞",  # Wooden entrance platform
            "Êµ¥ÊßΩÂè∞",  # Bath platform
            # Synonym testing (when we add synonyms back)
            "„Ç¶„Ç©„Ç∑„É•„É¨„ÉÉ„Éà",  # Should match Ê∏©Ê∞¥Ê¥óÊµÑ‰æøÂ∫ß
            "ËªäÊ§ÖÂ≠ê",  # Should match Ëªä„ÅÑ„Åô variants
            "Ëªä„Ç§„Çπ",  # Another wheelchair variant
            "Ëªä„ÅÑ„Åô",  # Yet another variant
            # Edge cases - numbers and special chars
            "22√ó1",  # Numbers with special chars
            "#3000",  # Hash + numbers
            "45W-30-1ÊÆµ",  # Complex alphanumeric
            # Long product names
            "„Å≤„ÅòÊéõ„Åë‰ªò„Ç∑„É£„ÉØ„Éº„Éô„É≥„ÉÅK-TH",  # Long descriptive name
            "Êäò„Çä„Åü„Åü„Åø„Ç∑„É£„ÉØ„Éº„Éô„É≥„ÉÅ",  # Foldable shower bench
            # Medical/care products
            "„Çπ„ÉÜ„ÇΩ„Çπ„Ç≥„Éº„Éó",  # Stethoscope
            "Ë°ÄÂúßË®à",  # Blood pressure monitor
            # Katakana brand names
            "„É™„ÉÉ„Éà„Éû„É≥",  # Littmann (brand)
            "„Ç≥„É©„É™„ÉÉ„ÉÅ",  # Collagen product
            # Partial matches that should work
            "„É°„Ç§„Ç∏",  # Brand prefix (might not exist)
            "„Ç∞„É©„É≥",  # GRAN series
            "„É´„Éü„Ç®",  # RUMIE series
            "„Çπ„Çø„Ç§„É´",  # Style series
            # Numbers only
            "3000",  # Should find #3000 items
            "45",  # Should find various 45* items
            # Common misspellings/variations
            "„Ç∑„É§„ÉØ„Éº",  # „É§ instead of „É£
            "„Éô„ÉÉ„Éà",  # „ÉÉ instead of „Éâ (bed)
            "„Éû„ÉÉ„Éà",  # Mat/mattress
            # English/Romaji (if romaji analyzer works)
            "shower",  # English for „Ç∑„É£„ÉØ„Éº
            "toilet",  # English for „Éà„Ç§„É¨
            # Non-existent (should return no results gracefully)
            "Â≠òÂú®„Åó„Å™„ÅÑÂïÜÂìÅ",  # Non-existent product
            "NONEXIST-999",  # Non-existent SKU
        ]

        for query in test_cases:
            try:
                # Test multiple field searches for better Japanese matching
                response = self.client.search(
                    index=self.index_name,
                    body={
                        "query": {
                            "bool": {
                                "should": [
                                    {"match": {"sku_name": query}},
                                    {"match": {"sku_name.ngram": query}},
                                    {"match": {"sku_name.fuzzy": query}},
                                    {"match": {"sku_name.partial": query}},
                                ]
                            }
                        },
                        "size": 5,
                    },
                )

                hits = len(response["hits"]["hits"])
                total = response["hits"]["total"]["value"]
                print(f"   '{query}': {hits} results (total: {total})")

                # Show top result for debugging
                if hits > 0:
                    top_result = response["hits"]["hits"][0]
                    score = top_result["_score"]
                    name = top_result["_source"]["sku_name"]
                    print(f"      ‚Üí Top: '{name}' (score: {score:.2f})")

            except Exception as e:
                print(f"   '{query}': Error - {e}")

        print("‚úÖ Index validation complete")

    def simple_search(self, query, max_results=10):
        """Simple fuzzy search method for testing"""
        try:
            response = self.client.search(
                index=self.index_name,
                body={
                    "query": {
                        "bool": {
                            "should": [
                                {"match": {"sku_name": {"query": query, "boost": 3.0}}},
                                {
                                    "match": {
                                        "sku_name.exact": {"query": query, "boost": 2.0}
                                    }
                                },
                                {
                                    "match": {
                                        "sku_name.ngram": {"query": query, "boost": 1.5}
                                    }
                                },
                                {
                                    "match": {
                                        "sku_name.fuzzy": {"query": query, "boost": 1.0}
                                    }
                                },
                                {
                                    "match": {
                                        "sku_name.partial": {
                                            "query": query,
                                            "boost": 1.2,
                                        }
                                    }
                                },
                                {
                                    "match": {
                                        "sku_name.synonym": {
                                            "query": query,
                                            "boost": 1.8,
                                        }
                                    }
                                },
                            ]
                        }
                    },
                    "size": max_results,
                },
            )

            hits = response["hits"]["hits"]
            total = response["hits"]["total"]["value"]

            print(f"\nüîç Search: '{query}'")
            print(f"üìä Found: {len(hits)} results (total: {total})")

            for i, hit in enumerate(hits, 1):
                name = hit["_source"]["sku_name"]
                score = hit["_score"]
                print(f"   {i:2d}. '{name}' (score: {score:.2f})")

            return hits

        except Exception as e:
            print(f"‚ùå Search failed: {e}")
            return []


def main():
    """Main indexing process"""
    print("üöÄ Japanese SKU Master Data Indexer")
    print("=" * 50)
    print("üìã Optimized for:")
    print("   - Japanese text variations (ÂÖ®Ëßí/ÂçäËßí)")
    print("   - Kanji/Hiragana/Katakana fuzzy matching")
    print("   - Multi-step AI search reasoning")
    print("   - Fast candidate retrieval")
    print("")

    indexer = JapaneseSKUIndexer()

    # Step 1: Connect
    if not indexer.connect():
        print("üí• Connection failed. Check troubleshooter.")
        return

    # Step 2: Create optimized index
    if not indexer.create_optimized_index():
        print("üí• Index creation failed.")
        return

    # Step 3: Index SKU data
    if not indexer.index_sku_data():
        print("üí• Data indexing failed.")
        return

    # Step 4: Validate
    indexer.validate_index()

    print("\nüéâ Indexing complete! Ready for fuzzy search testing.")

    # Optional: Interactive search mode
    while True:
        try:
            query = input("\nüîé Enter search query (or 'quit' to exit): ").strip()
            if query.lower() in ["quit", "exit", "q", ""]:
                break
            indexer.simple_search(query)
        except KeyboardInterrupt:
            break

    print("üëã Session ended.")


if __name__ == "__main__":
    main()
