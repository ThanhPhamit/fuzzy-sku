"""
OpenSearch Indexer for Japanese SKU Master Data
Optimized for fuzzy matching with Japanese text variations
"""

from opensearchpy import OpenSearch, RequestsHttpConnection
from requests_aws4auth import AWS4Auth
import boto3
import csv
import os
from datetime import datetime


class JapaneseSKUIndexer:
    def __init__(self):
        self.aws_profile = "welfan-lg-mfa"
        self.aws_region = "ap-northeast-3"
        self.endpoint = (
            "search-fuzzy-sku-ppba34qtds6ocweyl62wmgv5we.aos.ap-northeast-3.on.aws"
        )
        self.index_name = "sku-master"
        self.client = None

    def connect(self):
        """Connect to OpenSearch with AWS authentication"""
        try:
            print(f"üîß Connecting with profile: {self.aws_profile}")

            session = boto3.Session(profile_name=self.aws_profile)
            credentials = session.get_credentials()

            if not credentials:
                raise Exception(
                    f"AWS credentials not found for profile: {self.aws_profile}"
                )

            awsauth = AWS4Auth(
                credentials.access_key,
                credentials.secret_key,
                self.aws_region,
                "es",
                session_token=credentials.token,
            )

            self.client = OpenSearch(
                hosts=[{"host": self.endpoint, "port": 443}],
                http_auth=awsauth,
                use_ssl=True,
                verify_certs=True,
                connection_class=RequestsHttpConnection,
                timeout=30,
            )

            # Test connection
            health = self.client.cluster.health()
            print(f"‚úÖ Connected! Cluster: {health['status']}")
            return True

        except Exception as e:
            print(f"‚ùå Connection failed: {e}")
            return False

    def create_optimized_index(self):
        """Create index optimized for Japanese SKU fuzzy matching"""

        # Advanced mapping for Japanese text with multiple analyzers
        mapping = {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 1,
                "index.max_ngram_diff": 7,
                "analysis": {
                    "char_filter": {
                        "normalize_chars": {
                            "type": "mapping",
                            "mappings": [
                                # Zenkaku to Hankaku numbers
                                "Ôºê => 0",
                                "Ôºë => 1",
                                "Ôºí => 2",
                                "Ôºì => 3",
                                "Ôºî => 4",
                                "Ôºï => 5",
                                "Ôºñ => 6",
                                "Ôºó => 7",
                                "Ôºò => 8",
                                "Ôºô => 9",
                                # Zenkaku to Hankaku alphabets
                                "Ôº° => A",
                                "Ôº¢ => B",
                                "Ôº£ => C",
                                "Ôº§ => D",
                                "Ôº• => E",
                                "Ôº¶ => F",
                                "Ôºß => G",
                                "Ôº® => H",
                                "Ôº© => I",
                                "Ôº™ => J",
                                "Ôº´ => K",
                                "Ôº¨ => L",
                                "Ôº≠ => M",
                                "ÔºÆ => N",
                                "ÔºØ => O",
                                "Ôº∞ => P",
                                "Ôº± => Q",
                                "Ôº≤ => R",
                                "Ôº≥ => S",
                                "Ôº¥ => T",
                                "Ôºµ => U",
                                "Ôº∂ => V",
                                "Ôº∑ => W",
                                "Ôº∏ => X",
                                "Ôºπ => Y",
                                "Ôº∫ => Z",
                                # Lowercase versions
                                "ÔΩÅ => a",
                                "ÔΩÇ => b",
                                "ÔΩÉ => c",
                                "ÔΩÑ => d",
                                "ÔΩÖ => e",
                                "ÔΩÜ => f",
                                "ÔΩá => g",
                                "ÔΩà => h",
                                "ÔΩâ => i",
                                "ÔΩä => j",
                                "ÔΩã => k",
                                "ÔΩå => l",
                                "ÔΩç => m",
                                "ÔΩé => n",
                                "ÔΩè => o",
                                "ÔΩê => p",
                                "ÔΩë => q",
                                "ÔΩí => r",
                                "ÔΩì => s",
                                "ÔΩî => t",
                                "ÔΩï => u",
                                "ÔΩñ => v",
                                "ÔΩó => w",
                                "ÔΩò => x",
                                "ÔΩô => y",
                                "ÔΩö => z",
                                # Special characters
                                "Ôºà => (",
                                "Ôºâ => )",
                                "Ôºç => -",
                                "„ÄÄ => ",
                                "ÔΩû => ~",
                                "„Éª => „Éª",
                                "Ôºè => /",
                                "Ôºã => +",
                                "Ôºù => =",
                            ],
                        },
                        "katakana_hiragana": {
                            "type": "mapping",
                            "mappings": [
                                # ----- Goj≈´on -----
                                "„Ç¢ => „ÅÇ",
                                "„Ç§ => „ÅÑ",
                                "„Ç¶ => „ÅÜ",
                                "„Ç® => „Åà",
                                "„Ç™ => „Åä",
                                "„Ç´ => „Åã",
                                "„Ç≠ => „Åç",
                                "„ÇØ => „Åè",
                                "„Ç± => „Åë",
                                "„Ç≥ => „Åì",
                                "„Çµ => „Åï",
                                "„Ç∑ => „Åó",
                                "„Çπ => „Åô",
                                "„Çª => „Åõ",
                                "„ÇΩ => „Åù",
                                "„Çø => „Åü",
                                "„ÉÅ => „Å°",
                                "„ÉÑ => „Å§",
                                "„ÉÜ => „Å¶",
                                "„Éà => „Å®",
                                "„Éä => „Å™",
                                "„Éã => „Å´",
                                "„Éå => „Å¨",
                                "„Éç => „Å≠",
                                "„Éé => „ÅÆ",
                                "„Éè => „ÅØ",
                                "„Éí => „Å≤",
                                "„Éï => „Åµ",
                                "„Éò => „Å∏",
                                "„Éõ => „Åª",
                                "„Éû => „Åæ",
                                "„Éü => „Åø",
                                "„É† => „ÇÄ",
                                "„É° => „ÇÅ",
                                "„É¢ => „ÇÇ",
                                "„É§ => „ÇÑ",
                                "„É¶ => „ÇÜ",
                                "„É® => „Çà",
                                "„É© => „Çâ",
                                "„É™ => „Çä",
                                "„É´ => „Çã",
                                "„É¨ => „Çå",
                                "„É≠ => „Çç",
                                "„ÉØ => „Çè",
                                "„É≤ => „Çí",
                                "„É≥ => „Çì",
                                # ----- Dakuten / Handakuten -----
                                "„Ç¨ => „Åå",
                                "„ÇÆ => „Åé",
                                "„Ç∞ => „Åê",
                                "„Ç≤ => „Åí",
                                "„Ç¥ => „Åî",
                                "„Ç∂ => „Åñ",
                                "„Ç∏ => „Åò",
                                "„Ç∫ => „Åö",
                                "„Çº => „Åú",
                                "„Çæ => „Åû",
                                "„ÉÄ => „Å†",
                                "„ÉÇ => „Å¢",
                                "„ÉÖ => „Å•",
                                "„Éá => „Åß",
                                "„Éâ => „Å©",
                                "„Éê => „Å∞",
                                "„Éì => „Å≥",
                                "„Éñ => „Å∂",
                                "„Éô => „Åπ",
                                "„Éú => „Åº",
                                "„Éë => „Å±",
                                "„Éî => „Å¥",
                                "„Éó => „Å∑",
                                "„Éö => „Å∫",
                                "„Éù => „ÅΩ",
                                # ----- Small kana -----
                                "„Ç° => „ÅÅ",
                                "„Ç£ => „ÅÉ",
                                "„Ç• => „ÅÖ",
                                "„Çß => „Åá",
                                "„Ç© => „Åâ",
                                "„ÉÉ => „Å£",
                                "„É£ => „ÇÉ",
                                "„É• => „ÇÖ",
                                "„Éß => „Çá",
                                "„ÉÆ => „Çé",
                                # ----- Historical kana -----
                                "„É∞ => „Çê",
                                "„É± => „Çë",
                                # ----- V-sounds (modern; put longer first) -----
                                "„É¥„Ç° => „Çî„ÅÅ",
                                "„É¥„Ç£ => „Çî„ÅÉ",
                                "„É¥„Ç• => „Çî„ÅÖ",
                                "„É¥„Çß => „Çî„Åá",
                                "„É¥„Ç© => „Çî„Åâ",
                                "„É¥ => „Çî",
                                # Single-codepoint VA/VI/VE/VO ‚Üí modern
                                "„É∑ => „Çî„ÅÅ",
                                "„É∏ => „Çî„ÅÉ",
                                "„Éπ => „Çî„Åá",
                                "„É∫ => „Çî„Åâ",
                                # ----- Small KA/KE (counters; no auto-voicing) -----
                                "„Éµ => „Åã",
                                "„É∂ => „Åë",
                                # ----- Iteration marks (use kuromoji_iteration_mark if possible) -----
                                "„ÉΩ => „Çù",
                                "„Éæ => „Çû",
                                # ----- Keep-as-is for product formatting -----
                                "„Éº => „Éº",
                                "„Éª => „Éª",
                                # ----- Ainu small kana (Katakana Phonetic Extensions) -----
                                "„á∞ => „Åè",
                                "„á± => „Åó",
                                "„á≤ => „Åô",
                                "„á≥ => „Å®",
                                "„á¥ => „Å¨",
                                "„áµ => „ÅØ",
                                "„á∂ => „Å≤",
                                "„á∑ => „Åµ",
                                "„á∑„Çö => „Å∑",
                                "„á∏ => „Å∏",
                                "„áπ => „Åª",
                                "„á∫ => „ÇÄ",
                                "„áª => „Çâ",
                                "„áº => „Çä",
                                "„áΩ => „Çã",
                                "„áæ => „Çå",
                                "„áø => „Çç",
                            ],
                        },
                    },
                    "tokenizer": {
                        "japanese_char_ngram": {
                            "type": "ngram",
                            "min_gram": 2,
                            "max_gram": 4,
                            "token_chars": ["letter", "digit"],
                        }
                    },
                    "analyzer": {
                        # Standard Japanese analyzer - for basic word-level matching
                        # Normalizes text and uses Kuromoji for proper Japanese tokenization
                        "japanese_standard": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_baseform",  # Convert to dictionary form
                                "kuromoji_part_of_speech",  # Filter by POS tags
                                "cjk_width",  # Normalize character width
                                "lowercase",
                            ],
                        },
                        # N-gram analyzer - for prefix/autocomplete matching
                        # Good for "as-you-type" search functionality
                        "japanese_ngram": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_baseform",
                                "cjk_width",
                                "lowercase",
                                "edge_ngram_filter",  # Creates prefix tokens (2-8 chars)
                            ],
                        },
                        # Character-level fuzzy analyzer - for typo tolerance
                        # Handles character-level variations and misspellings
                        "japanese_fuzzy": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "japanese_char_ngram",  # Character n-grams
                            "filter": [
                                "cjk_width",
                                "lowercase",
                            ],
                        },
                        # Partial word matching - for incomplete queries
                        # Allows matching parts of words within compound terms
                        "japanese_partial": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_baseform",
                                "cjk_width",
                                "lowercase",
                                "char_ngram_filter",  # Creates 2-4 char n-grams
                            ],
                        },
                        # Exact match analyzer - for precise queries
                        # Treats entire input as single token for exact matching
                        "exact_match": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "keyword",  # No tokenization, exact match
                            "filter": ["cjk_width", "lowercase"],
                        },
                        # Reading analyzer - for phonetic matching
                        # Useful for matching different writings of same pronunciation
                        "reading_analyzer": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_baseform",
                                "cjk_width",
                                "lowercase",
                            ],
                        },
                        # Synonym analyzer - for domain-specific term matching
                        # Expands queries with related medical/care product terms
                        "synonym_analyzer": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_baseform",
                                "cjk_width",
                                "lowercase",
                                "product_synonyms",  # Applies synonym mappings
                            ],
                        },
                        # Romaji analyzer - for English/ASCII input matching
                        # Converts Japanese (Hiragana/Katakana/Kanji) to Latin alphabet (Romaji)
                        "romaji_analyzer": {
                            "type": "custom",
                            "char_filter": ["normalize_chars", "katakana_hiragana"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_baseform",
                                "kuromoji_readingform",  # Converts to Katakana reading
                                "romaji_readingform",  # Converts Katakana ‚Üí Romaji
                                "cjk_width",
                                "lowercase",
                            ],
                        },
                        # Pure Romaji converter - converts everything to Latin alphabet
                        # Best for cross-language matching (Japanese input ‚Üí English output)
                        "to_romaji_analyzer": {
                            "type": "custom",
                            "char_filter": ["normalize_chars"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_readingform",  # Kanji ‚Üí Katakana („Ç∑„É£„ÉØ„Éº)
                                "romaji_readingform",  # Katakana ‚Üí Romaji (shawaa)
                                "cjk_width",
                                "lowercase",
                            ],
                        },
                        # Latin N-gram analyzer - for substring matching in Latin text
                        # KEY SOLUTION: Index "MOGU" ‚Üí creates n-grams: "mo", "og", "gu", "mog", "ogu", "mogu"
                        # Then search "„ÇÇ„Åê„Å£„Å°" ‚Üí converts to "mogucchi" ‚Üí matches "mogu" n-gram
                        "latin_ngram_analyzer": {
                            "type": "custom",
                            "char_filter": ["normalize_chars"],
                            "tokenizer": "standard",  # Standard tokenizer for Latin text
                            "filter": [
                                "lowercase",
                                "latin_ngram_filter",  # Creates n-grams for Latin text
                            ],
                        },
                        # Romaji Edge N-gram analyzer - for prefix matching on Romaji
                        # Solves: "„ÇÇ„Åê„Å£„Å°" (mogucchi) should match "MOGU" (mogu) as prefix
                        # Index: "mogucchi" ‚Üí edge n-grams ["m", "mo", "mog", "mogu", "moguc", "mogucc", "mogucch", "mogucchi"]
                        # Search: "mogu" ‚Üí matches edge n-gram "mogu"
                        "romaji_edge_ngram_analyzer": {
                            "type": "custom",
                            "char_filter": ["normalize_chars"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_readingform",  # Convert to Katakana reading
                                "romaji_readingform",  # Convert to Romaji
                                "lowercase",
                                "romaji_edge_ngram_filter",  # Create edge n-grams
                            ],
                        },
                        # Standard analyzer for Romaji search (no n-gram at search time)
                        "romaji_search_analyzer": {
                            "type": "custom",
                            "char_filter": ["normalize_chars"],
                            "tokenizer": "kuromoji_tokenizer",
                            "filter": [
                                "kuromoji_readingform",
                                "romaji_readingform",
                                "lowercase",
                            ],
                        },
                    },
                    "filter": {
                        # Edge n-gram filter - creates prefix tokens for autocomplete
                        # Generates tokens like: "„Ç∑„É£", "„Ç∑„É£„ÉØ", "„Ç∑„É£„ÉØ„Éº" from "„Ç∑„É£„ÉØ„Éº"
                        "edge_ngram_filter": {
                            "type": "edge_ngram",
                            "min_gram": 2,
                            "max_gram": 8,
                        },
                        # Character n-gram filter - creates overlapping character sequences
                        # Generates tokens like: "„Ç∑„É£", "„É£„ÉØ", "„ÉØ„Éº" from "„Ç∑„É£„ÉØ„Éº"
                        "char_ngram_filter": {
                            "type": "ngram",
                            "min_gram": 2,
                            "max_gram": 4,
                        },
                        # Kuromoji reading form - converts Kanji to Katakana reading
                        # Example: ËªäÊ§ÖÂ≠ê ‚Üí „ÇØ„É´„Éû„Ç§„Çπ (phonetic reading)
                        "kuromoji_readingform": {
                            "type": "kuromoji_readingform",
                            "use_romaji": False,  # First convert to Katakana
                        },
                        # Romaji reading form - converts Katakana to Latin alphabet
                        # Example: „Ç∑„É£„ÉØ„Éº ‚Üí shawaa, „Éà„Ç§„É¨ ‚Üí toire
                        "romaji_readingform": {
                            "type": "kuromoji_readingform",
                            "use_romaji": True,  # Convert to Romaji (Latin alphabet)
                        },
                        # Latin N-gram filter - creates n-grams for Latin/ASCII text
                        # Example: "MOGU" ‚Üí ["mo", "og", "gu", "mog", "ogu", "mogu"]
                        # This allows "mogu" (from „ÇÇ„Åê„Å£„Å°) to match "MOGU"
                        "latin_ngram_filter": {
                            "type": "ngram",
                            "min_gram": 2,
                            "max_gram": 6,  # Support longer brand names
                        },
                        # Romaji Edge N-gram filter - creates prefix n-grams for Romaji
                        # Example: "mogucchi" ‚Üí ["m", "mo", "mog", "mogu", "moguc", "mogucc", "mogucch", "mogucchi"]
                        # Allows prefix search: "mogu" matches "mogucchi"
                        "romaji_edge_ngram_filter": {
                            "type": "edge_ngram",
                            "min_gram": 2,
                            "max_gram": 10,  # Support longer Japanese words in Romaji
                        },
                        # Product synonym filter - expands medical/care product terminology
                        # Maps related terms: "ËªäÊ§ÖÂ≠ê" ‚Üî "Ëªä„ÅÑ„Åô" ‚Üî "Ëªä„Ç§„Çπ" ‚Üî "„Ç¶„Ç£„Éº„É´„ÉÅ„Çß„Ç¢"
                        "product_synonyms": {
                            "type": "synonym",
                            "synonyms": [
                                "‰ªãË≠∑Áî®„Åä„ÇÄ„Å§,Â§ß‰∫∫Áî®„Åä„ÇÄ„Å§,Â§±Á¶ÅÁî®„Åä„ÇÄ„Å§,„Ç¢„ÉÄ„É´„Éà„ÉÄ„Ç§„Éë„Éº,Á¥ô„Åä„ÇÄ„Å§",
                                "Â∞øÂèñ„Çä„Éë„ÉÉ„Éâ,Â∞ø„Å®„Çä„Éë„ÉÉ„Éâ,Â§±Á¶Å„Éë„ÉÉ„Éâ,‰ªãË≠∑„Éë„ÉÉ„Éâ",
                                "„Éù„Éº„Çø„Éñ„É´„Éà„Ç§„É¨,Á∞°Êòì„Éà„Ç§„É¨,‰ªãË≠∑„Éà„Ç§„É¨,ÁßªÂãïÂºè„Éà„Ç§„É¨",
                                "Ê∏©Ê∞¥Ê¥óÊµÑ‰æøÂ∫ß,„Ç¶„Ç©„Ç∑„É•„É¨„ÉÉ„Éà,„Ç∑„É£„ÉØ„Éº„Éà„Ç§„É¨",
                                "‰æøÂô®,‰æøÂ∫ß,‰æøÂ∫ßÂÆπÂô®,„Éô„ÉÉ„Éâ„Éë„É≥",
                                "Ëªä„ÅÑ„Åô,ËªäÊ§ÖÂ≠ê,Ëªä„Ç§„Çπ,„Ç¶„Ç£„Éº„É´„ÉÅ„Çß„Ç¢",
                                "Ê≠©Ë°åÂô®,„Ç∑„É´„Éê„Éº„Ç´„Éº,„É≠„É¨„Éº„Çø,„É≠„Éº„É©„Éº„Çø,„É≠„Éº„É©„Éà„Éº„É´",
                                "Êùñ,„Å§„Åà,„Çπ„ÉÜ„ÉÉ„Ç≠,Ê≠©Ë°åÊùñ",
                                "Áßª‰πóÁî®„É™„Éï„Éà,‰ªãË≠∑„É™„Éï„Éà,„É™„Éï„Çø„Éº,„Å§„Çä‰∏ä„Åí„É™„Éï„Éà",
                                "„Ç∑„Éã„Ç¢„Ç´„Éº,ÈõªÂãï„Ç∑„Éã„Ç¢„Ç´„Éº,ÈõªÂãï„Ç´„Éº„Éà,„É¢„Éì„É™„ÉÜ„Ç£„Çπ„ÇØ„Éº„Çø„Éº",
                                "‰ªãË≠∑„Éô„ÉÉ„Éâ,‰ªãË≠∑Áî®„Éô„ÉÉ„Éâ,ÈõªÂãï„Éô„ÉÉ„Éâ,„É™„ÇØ„É©„Ç§„Éã„É≥„Ç∞„Éô„ÉÉ„Éâ",
                                "‰ΩìÂúßÂàÜÊï£„Éû„ÉÉ„Éà„É¨„Çπ,„Ç®„Ç¢„Éû„ÉÉ„Éà„É¨„Çπ,Ë§•Áò°‰∫àÈò≤„Éû„ÉÉ„Éà„É¨„Çπ,Ë§•Áò°„Éû„ÉÉ„Éà",
                                "Èõ¢Â∫ä„Çª„É≥„Çµ„Éº,Ë¶ãÂÆà„Çä„Çª„É≥„Çµ„Éº,ÂæòÂæä„Çª„É≥„Çµ„Éº,Ëµ∑„Åç‰∏ä„Åå„Çä„Çª„É≥„Çµ„Éº",
                                "„Ç∑„É£„ÉØ„Éº„ÉÅ„Çß„Ç¢,ÂÖ•Êµ¥Áî®„ÅÑ„Åô,ÂÖ•Êµ¥Ê§ÖÂ≠ê,È¢®ÂëÇ„ÅÑ„Åô",
                                "Âè£ËÖî„Ç±„Ç¢,Âè£ËÖîÊ∏ÖÊã≠,Âè£ËÖîÁî®„Çπ„Éù„É≥„Ç∏,„Ç™„Éº„É©„É´„Ç±„Ç¢",
                                "‰Ωø„ÅÑÊç®„Å¶ÊâãË¢ã,‰Ωø„ÅÑÂàá„ÇäÊâãË¢ã,„Éã„Éà„É™„É´ÊâãË¢ã,„É©„ÉÜ„ÉÉ„ÇØ„ÇπÊâãË¢ã,„Éì„Éã„Éº„É´ÊâãË¢ã",
                                "„Éû„Çπ„ÇØ,„Çµ„Éº„Ç∏„Ç´„É´„Éû„Çπ„ÇØ,‰ªãË≠∑Áî®„Éû„Çπ„ÇØ,‰∏çÁπîÂ∏É„Éû„Çπ„ÇØ",
                                "Ê∂àÊØíÊ∂≤,„Ç¢„É´„Ç≥„Éº„É´Ê∂àÊØí,Èô§ËèåÊ∂≤,„Ç®„Çø„Éé„Éº„É´Ê∂àÊØí",
                                "‰ΩìÊ∏©Ë®à,„Éá„Ç∏„Çø„É´‰ΩìÊ∏©Ë®à,ÈùûÊé•Ëß¶‰ΩìÊ∏©Ë®à,„Åß„ÅìÊ∏©Â∫¶Ë®à",
                                "Ë°ÄÂúßË®à,‰∏äËÖïÂºèË°ÄÂúßË®à,ÊâãÈ¶ñÂºèË°ÄÂúßË®à",
                                "„Éë„É´„Çπ„Ç™„Ç≠„Ç∑„É°„Éº„Çø„Éº,„Éë„É´„Çπ„Ç™„Ç≠„Ç∑„É°„Éº„Çø,Ë°Ä‰∏≠ÈÖ∏Á¥†ÊøÉÂ∫¶Ë®à,SpO2Ë®à",
                                "„Å®„Çç„ÅøÂâ§,Â¢óÁ≤òÂâ§,„Éà„É≠„ÉüÂâ§",
                                "Ê†ÑÈ§äË£úÂä©È£üÂìÅ,‰ªãË≠∑È£ü,„ÇΩ„Éï„ÉàÈ£ü,„Éü„Ç≠„Çµ„ÉºÈ£ü",
                                "Ëá™Âãï,„Ç™„Éº„Éà,„Ç∏„Éâ„Ç¶,„Ç™„Éº„Éà„Éû„ÉÅ„ÉÉ„ÇØ",
                                "„Çª„É≥„Çµ„Éº,ÊÑüÁü•Âô®,„Çª„É≥„Çµ,Ëµ§Â§ñÁ∑ö„Çª„É≥„Çµ„Éº",
                                "„Éù„Éº„Çø„Éñ„É´,ÊåÅ„Å°ÈÅã„Å≥,ÁßªÂãïÂºè,Êê∫Â∏Ø",
                                "Â∫ä„Åö„ÇåÈò≤Ê≠¢,Ë§•Áò°‰∫àÈò≤,„Åò„Çá„Åè„Åù„ÅÜ‰∫àÈò≤",
                            ],
                        },
                    },
                },
            },
            "mappings": {
                "properties": {
                    # Main SKU name field with multiple sub-fields for different search strategies
                    "sku_name": {
                        "type": "text",
                        "analyzer": "japanese_standard",  # Default analyzer for indexing
                        "search_analyzer": "japanese_partial",  # Different analyzer for search queries
                        "fields": {
                            # Exact match field - for precise queries (highest priority)
                            "exact": {"type": "text", "analyzer": "exact_match"},
                            # N-gram field - for prefix/autocomplete matching
                            "ngram": {"type": "text", "analyzer": "japanese_ngram"},
                            # Fuzzy field - for character-level typo tolerance
                            "fuzzy": {"type": "text", "analyzer": "japanese_fuzzy"},
                            # Partial field - for incomplete word matching
                            "partial": {"type": "text", "analyzer": "japanese_partial"},
                            # Synonym field - for domain-specific term expansion
                            "synonym": {"type": "text", "analyzer": "synonym_analyzer"},
                            # Romaji field - for English/ASCII cross-language search
                            # Allows searching Japanese products using English alphabet
                            # Example: "shawaa" can match "„Ç∑„É£„ÉØ„Éº"
                            "romaji": {"type": "text", "analyzer": "romaji_analyzer"},
                            # Pure Latin alphabet conversion - stores Japanese as Romaji
                            # Example: "ËªäÊ§ÖÂ≠ê" ‚Üí "kurumaisu", "„Éà„Ç§„É¨" ‚Üí "toire"
                            "latin": {"type": "text", "analyzer": "to_romaji_analyzer"},
                            # Latin N-gram field - KEY SOLUTION for Japanese ‚Üí Latin matching
                            # Indexes Latin text with n-grams: "MOGU" ‚Üí ["mo","og","gu","mog","ogu","mogu"]
                            # Allows "„ÇÇ„Åê„Å£„Å°"‚Üí"mogucchi" to match "MOGU"‚Üí"mogu" n-gram
                            "latin_ngram": {
                                "type": "text",
                                "analyzer": "latin_ngram_analyzer",
                            },
                            # Romaji Edge N-gram field - CRITICAL for prefix matching
                            # Indexes Romaji with edge n-grams: "mogucchi" ‚Üí ["mo","mog","mogu","moguc"...]
                            # Allows partial search: "„ÇÇ„Åê„Å£„Å°"‚Üí"mogu" to match full "MOGU" brand
                            "romaji_ngram": {
                                "type": "text",
                                "analyzer": "romaji_edge_ngram_analyzer",
                                "search_analyzer": "romaji_search_analyzer",
                            },
                            # Keyword field - for aggregations and exact filtering
                            "keyword": {"type": "keyword", "ignore_above": 256},
                        },
                    },
                    # Timestamp field - when this SKU was indexed
                    "indexed_at": {"type": "date"},
                }
            },
        }

        try:
            if self.client.indices.exists(index=self.index_name):
                print(f"‚ö†Ô∏è  Index '{self.index_name}' exists")
                choice = input("Delete and recreate? (y/N): ").lower()
                if choice in ["y", "yes"]:
                    self.client.indices.delete(index=self.index_name)
                    print(f"üóëÔ∏è  Deleted: {self.index_name}")
                else:
                    return True

            self.client.indices.create(index=self.index_name, body=mapping)
            print(f"‚úÖ Created optimized index: {self.index_name}")
            return True

        except Exception as e:
            print(f"‚ùå Index creation failed: {e}")
            return False

    def index_sku_data(self, csv_file="TM_SYOHIN_202509302313.csv"):
        """Index SKU master data from CSV with batch processing"""

        if not os.path.exists(csv_file):
            print(f"‚ùå File not found: {csv_file}")
            return False

        try:
            print(f"üìÑ Reading: {csv_file}")
            products = []

            with open(csv_file, "r", encoding="utf-8") as file:
                reader = csv.reader(file)
                header = next(reader, None)
                print(f"Header: {header}")

                for row_id, row in enumerate(reader, start=1):
                    if row and row[0].strip():
                        sku_name = row[0].strip()

                        product = {
                            "sku_name": sku_name,
                            "indexed_at": datetime.now().isoformat(),
                        }
                        products.append(product)

            print(f"üìä Loaded {len(products)} SKU records")

            # Bulk index with progress tracking
            batch_size = 100
            total_indexed = 0

            for i in range(0, len(products), batch_size):
                batch = products[i : i + batch_size]
                bulk_body = []

                for idx, product in enumerate(batch, start=i + 1):
                    bulk_body.extend(
                        [
                            {
                                "index": {
                                    "_index": self.index_name,
                                    "_id": idx,  # Use sequential index as ID
                                }
                            },
                            product,
                        ]
                    )

                response = self.client.bulk(body=bulk_body)

                # Check for errors
                errors = 0
                if response.get("errors"):
                    for item in response["items"]:
                        if "index" in item and "error" in item["index"]:
                            errors += 1
                            print(
                                f"   Error ID {item['index']['_id']}: {item['index']['error']['reason']}"
                            )

                total_indexed += len(batch) - errors
                batch_num = (i // batch_size) + 1
                print(
                    f"üìù Batch {batch_num}: {len(batch) - errors}/{len(batch)} indexed (Total: {total_indexed})"
                )

            # Refresh index for immediate search
            self.client.indices.refresh(index=self.index_name)
            print(f"üéâ Successfully indexed {total_indexed} SKU records")

            # Show index statistics
            stats = self.client.indices.stats(index=self.index_name)
            doc_count = stats["indices"][self.index_name]["total"]["docs"]["count"]
            size = stats["indices"][self.index_name]["total"]["store"]["size_in_bytes"]
            print(f"üìà Index stats: {doc_count} docs, {size:,} bytes")

            return True

        except Exception as e:
            print(f"‚ùå Indexing failed: {e}")
            return False

    def validate_index(self):
        """Validate indexed data with sample searches"""
        print("\nüîç Validating index with sample searches...")

        test_cases = []

        for query in test_cases:
            try:
                # Test multiple field searches for better Japanese matching
                response = self.client.search(
                    index=self.index_name,
                    body={
                        "query": {
                            "bool": {
                                "should": [
                                    {"match": {"sku_name": query}},
                                    {"match": {"sku_name.ngram": query}},
                                    {"match": {"sku_name.fuzzy": query}},
                                    {"match": {"sku_name.partial": query}},
                                ]
                            }
                        },
                        "size": 5,
                    },
                )

                hits = len(response["hits"]["hits"])
                total = response["hits"]["total"]["value"]
                print(f"   '{query}': {hits} results (total: {total})")

                # Show top result for debugging
                if hits > 0:
                    top_result = response["hits"]["hits"][0]
                    score = top_result["_score"]
                    name = top_result["_source"]["sku_name"]
                    print(f"      ‚Üí Top: '{name}' (score: {score:.2f})")

            except Exception as e:
                print(f"   '{query}': Error - {e}")

        print("‚úÖ Index validation complete")

    def simple_search(self, query, max_results=10):
        """
        Production-ready search for Japanese SKU matching
        Optimized for bi-directional Japanese ‚Üî Latin matching
        """
        try:
            # ===== STRATEGY 1: Japanese Field Matching =====
            # Search in main Japanese fields with various analyzers
            japanese_queries = [
                # Main field - highest boost for exact matches
                {"match": {"sku_name": {"query": query, "boost": 5.0}}},
                # Exact match - for precise queries
                {"match": {"sku_name.exact": {"query": query, "boost": 4.0}}},
                # N-gram - for prefix/autocomplete matching
                {"match": {"sku_name.ngram": {"query": query, "boost": 3.0}}},
                # Fuzzy - for character-level typo tolerance
                {"match": {"sku_name.fuzzy": {"query": query, "boost": 2.5}}},
                # Partial - for incomplete word matching
                {"match": {"sku_name.partial": {"query": query, "boost": 3.0}}},
                # Synonym - for domain-specific term expansion
                {"match": {"sku_name.synonym": {"query": query, "boost": 2.5}}},
            ]

            # ===== STRATEGY 2: Romaji/Latin Field Matching =====
            # These fields use analyzers that convert Japanese ‚Üí Romaji at INDEX time
            # Query will be converted by field's search_analyzer automatically
            # NO need to set "analyzer" here - let OpenSearch use field's analyzer!
            romaji_queries = [
                # Romaji field - Japanese text indexed as romaji
                {"match": {"sku_name.romaji": {"query": query, "boost": 3.0}}},
                # Latin field - pure romaji conversion
                {"match": {"sku_name.latin": {"query": query, "boost": 3.0}}},
                # Latin N-gram - KEY for substring matching („ÇÇ„Åê„Å£„Å° ‚Üí MOGU)
                # This is the most important field for Japanese ‚Üí Latin brand matching
                {"match": {"sku_name.latin_ngram": {"query": query, "boost": 5.0}}},
                # Romaji Edge N-gram - CRITICAL for prefix matching („ÇÇ„Åê„Å£„Å° ‚Üí MOGU)
                # Allows partial Japanese input to match Latin brand names
                # Example: "„ÇÇ„Åê„Å£„Å°"‚Üí"mogu" matches "MOGU" brand via edge n-grams
                {"match": {"sku_name.romaji_ngram": {"query": query, "boost": 4.0}}},
            ]

            # ===== STRATEGY 3: Wildcard/Fuzzy Fallback (Low Priority) =====
            # Only used as last resort for edge cases
            # Keep boost low (1.0-1.5) to avoid performance issues
            # NOTE: Consider removing wildcard if index scales beyond 100k documents
            fallback_queries = [
                # Fuzzy matching for typos
                {
                    "fuzzy": {
                        "sku_name": {"value": query, "fuzziness": "AUTO", "boost": 1.2}
                    }
                },
            ]

            # Combine all strategies
            should_queries = []
            should_queries.extend(japanese_queries)
            should_queries.extend(romaji_queries)
            should_queries.extend(fallback_queries)

            response = self.client.search(
                index=self.index_name,
                body={
                    "query": {
                        "bool": {
                            "should": should_queries,
                            "minimum_should_match": 1,
                        }
                    },
                    "size": max_results,
                },
            )

            hits = response["hits"]["hits"]
            total = response["hits"]["total"]["value"]

            print(f"\nüîç Search: '{query}'")
            print(f"üìä Found: {len(hits)} results (total: {total})")

            for i, hit in enumerate(hits, 1):
                name = hit["_source"]["sku_name"]
                score = hit["_score"]
                print(f"   {i:2d}. '{name}' (score: {score:.2f})")

            return hits

        except Exception as e:
            print(f"‚ùå Search failed: {e}")
            return []


def main():
    """Main indexing process"""
    print("üöÄ Japanese SKU Master Data Indexer")
    print("=" * 50)
    print("üìã Optimized for:")
    print("   - Japanese text variations (ÂÖ®Ëßí/ÂçäËßí)")
    print("   - Kanji/Hiragana/Katakana fuzzy matching")
    print("   - Multi-step AI search reasoning")
    print("   - Fast candidate retrieval")
    print("")

    indexer = JapaneseSKUIndexer()

    # Step 1: Connect
    if not indexer.connect():
        print("üí• Connection failed. Check troubleshooter.")
        return

    # Step 2: Create optimized index
    if not indexer.create_optimized_index():
        print("üí• Index creation failed.")
        return

    # Step 3: Index SKU data
    if not indexer.index_sku_data():
        print("üí• Data indexing failed.")
        return

    # Step 4: Validate
    indexer.validate_index()

    print("\nüéâ Indexing complete! Ready for fuzzy search testing.")

    # Optional: Interactive search mode
    while True:
        try:
            query = input("\nüîé Enter search query (or 'quit' to exit): ").strip()
            if query.lower() in ["quit", "exit", "q", ""]:
                break
            indexer.simple_search(query)
        except KeyboardInterrupt:
            break

    print("üëã Session ended.")


if __name__ == "__main__":
    main()
